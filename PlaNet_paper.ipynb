{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weLL-G16j5G0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f2RSMcRCkc8B"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, in_channels=3):\n",
    "    super().__init__()\n",
    "    # 64 x 64 x 3\n",
    "    # 0 = (I - K + 2p)/s\n",
    "    self.cv1 = nn.Conv2d(in_channels, 32, 4, 2, 1) # 32 x 32 x 32\n",
    "    self.cv2 = nn.Conv2d(32, 64, 4, 2, 1) # 64 x 16 x 16\n",
    "    self.cv3 = nn.Conv2d(64, 128, 4, 2, 1) # 128 x 8 x 8\n",
    "    self.cv4 = nn.Conv2d(128, 256, 4, 2, 1) # 256 x 4 x 4\n",
    "    self.fn = nn.Linear(256*4*4, 1024)\n",
    "  def forward(self, x):\n",
    "    x = x.permute(0, 3, 1, 2) # batch x 3 x 64 x 64\n",
    "    x = F.relu(self.cv1(x))\n",
    "    x = F.relu(self.cv2(x))\n",
    "    x = F.relu(self.cv3(x))\n",
    "    x = F.relu(self.cv4(x))\n",
    "    x = x.reshape(x.size(0), -1)\n",
    "    x = self.fn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbg1ifmUcPbn",
    "outputId": "ba1d7987-5b7e-4302-92f3-922d777262d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "img = torch.rand([1, 64, 64, 3])\n",
    "enc = Encoder(3)\n",
    "print(enc(img).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34075e5f"
   },
   "source": [
    "### GRU at Time Step $t$\n",
    "\n",
    "**Inputs:**\n",
    "*   $x_t$: Current input\n",
    "*   $h_{t-1}$: Previous hidden state\n",
    "\n",
    "**Equations:**\n",
    "\n",
    "*   **Reset gate ($r_t$):** Controls how much past state is used to form new content.\n",
    "    $$r_t = \\sigma(W_r x_t + U_r h_{t-1})$$\n",
    "\n",
    "*   **Update gate ($z_t$):** Controls how much of the old state is kept.\n",
    "    $$z_t = \\sigma(W_z x_t + U_z h_{t-1})$$\n",
    "\n",
    "*   **Candidate hidden state ($h_t$):** An RNN-like state, gated by the reset gate.\n",
    "    $$h_t = \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}))$$\n",
    "\n",
    "*   **Final hidden state ($h_t$):** Interpolation between the old and new candidate states.\n",
    "    $$h_t = z_t \\odot h_{t-1} + (1 - z_t) \\odot \\tilde{h}_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCR5Akz1pGzM"
   },
   "outputs": [],
   "source": [
    "\n",
    "# \"\"\"\n",
    "# The reset gate determines how much of the old state is used to construct new information.\n",
    "# The update gate determines how much of the old state is kept versus replaced.\n",
    "# \"\"\"\n",
    "# class GRU(nn.Module):\n",
    "#   def __init__(self, input_size, hidden_size=200):\n",
    "#     super().__init__()\n",
    "#     hidden_size = hidden_size\n",
    "#     self.wr = nn.Linear(input_size, hidden_size)\n",
    "#     self.ur = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "#     self.wz = nn.Linear(input_size, hidden_size)\n",
    "#     self.uz = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "\n",
    "#     self.wh = nn.Linear(input_size, hidden_size)\n",
    "#     self.uh = nn.Linear(hidden_size, hidden_size)\n",
    "#     self.tanh = nn.Tanh()\n",
    "#     self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#   def forward(self, x, h_old):\n",
    "#     r = self.sigmoid(self.wr(x) + self.ur(h_old))\n",
    "#     z = self.sigmoid(self.wz(x) + self.uz(h_old))\n",
    "#     h_beta = self.tanh(self.wh(x) + self.uh(r*h_old))\n",
    "#     h = z* h_old + (1 - z)*h_beta\n",
    "#     return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JdRgs1cK18G"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "  def __init__(self, state_size=30, action_dim=4, hidden_size=200):\n",
    "    super().__init__()\n",
    "    input_size = state_size + action_dim\n",
    "    self.gru = nn.GRUCell(input_size, hidden_size)\n",
    "  def forward(self, s_t, a_t, h_old):\n",
    "    x = torch.cat([s_t, a_t], dim=-1)\n",
    "    return self.gru(x, h_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AtWqOMLRSH5B"
   },
   "outputs": [],
   "source": [
    "class Posterior(nn.Module):\n",
    "  def __init__(self, input_size=1224, output = 30):\n",
    "    super().__init__()\n",
    "    # 1024 + 200 -> e_t + h_t\n",
    "    input_size = 1024 + 200\n",
    "    self.fc = nn.Linear(input_size, 256)\n",
    "    self.fc_mu = nn.Linear(256, output)\n",
    "    self.fc_std = nn.Linear(256, output)\n",
    "\n",
    "  def forward(self, e_t, h_t):\n",
    "    eps = 0.1 # for stability and ensure model is never 100% confident\n",
    "    x = torch.cat([e_t, h_t], dim=-1)\n",
    "    x = F.relu(self.fc(x))\n",
    "    mean = self.fc_mu(x)\n",
    "    std = F.softplus(self.fc_std(x)) + eps\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "V6PSVkTGWrfn"
   },
   "outputs": [],
   "source": [
    "class Prior(nn.Module):\n",
    "  def __init__(self, hidden_size=200, output_size=30):\n",
    "    super().__init__()\n",
    "    self.fc = nn.Linear(hidden_size, 256)\n",
    "    self.fc_mu = nn.Linear(256, output_size)\n",
    "    self.fc_std = nn.Linear(256, output_size)\n",
    "  def forward(self, h):\n",
    "    eps = 0.1\n",
    "    x = F.relu(self.fc(h))\n",
    "    mu = self.fc_mu(x)\n",
    "    std = F.softplus(self.fc_std(x)) + eps\n",
    "\n",
    "    return mu, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2RkHKj4Wa--q"
   },
   "outputs": [],
   "source": [
    "class RSSM(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder()\n",
    "    self.gru = GRU()\n",
    "    self.posterior = Posterior()\n",
    "    self.prior = Prior()\n",
    "\n",
    "\n",
    "  def obs_step(self, h_old, s_old, obs, a_t):\n",
    "    h = self.gru(s_old, a_t, h_old)\n",
    "    e = self.encoder(obs)\n",
    "    m_pr, std_pr = self.prior(h)\n",
    "    m_po, std_po = self.posterior(e, h)\n",
    "    s = m_po + std_po*torch.randn_like(m_po)\n",
    "\n",
    "    return m_pr, std_pr, m_po, std_po, h, s\n",
    "\n",
    "\n",
    "  def imagine_step(self, h_old, s_old, a_t):\n",
    "    h = self.gru(s_old, a_t, h_old)\n",
    "    m_pr, std_pr = self.prior(h)\n",
    "    s = m_pr + std_pr * torch.randn_like(m_pr)\n",
    "\n",
    "    return m_pr, std_pr, h, s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuvlUGOYqLOc"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, state_size=30, hidden_size=200):\n",
    "    \"\"\"\n",
    "    output shape of conv2dTranspose -> 0 = (I - 1)*s-2P+k\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    input_size = state_size +  hidden_size\n",
    "    self.fc1 = nn.Linear(input_size, 4096) # 256*4*4\n",
    "    self.dec1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1) # 256 x 4 x 4 -> 128 x 8 x 8\n",
    "    self.dec2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, ) # 128 x 8 x 8 -> 64 x 16 x 16\n",
    "    self.dec3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, ) # 64 x 16 x 16 -> 32 x 32 x 32\n",
    "    self.dec4 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1, ) # 32 x 32 x 32-> 3 x 64 x 64\n",
    "\n",
    "  def forward(self, h, s):\n",
    "    x = torch.cat([h, s], dim=-1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = x.reshape(-1, 256, 4, 4)\n",
    "    x = F.relu(self.dec1(x))\n",
    "    x = F.relu(self.dec2(x))\n",
    "    x = F.relu(self.dec3(x))\n",
    "    x = torch.sigmoid(self.dec4(x))\n",
    "    obs = x.permute(0, 2, 3, 1)\n",
    "\n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSbu5mkMzRhg",
    "outputId": "8c9f31d1-772d-4a4a-bee1-ae0ec7b0c695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "h = torch.rand(200)\n",
    "s = torch.rand(30)\n",
    "dec = Decoder()\n",
    "img = dec(h, s)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "HdprMCuRyqh0"
   },
   "outputs": [],
   "source": [
    "class Reward(nn.Module):\n",
    "  def __init__(self, s_size=30, hidden_size=200, hidden_dim=400):\n",
    "    super().__init__()\n",
    "    input_size=s_size + hidden_size\n",
    "    self.fc1 = nn.Linear(input_size, hidden_dim)\n",
    "    self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "  def forward(self, h, s):\n",
    "    x = torch.cat([s, h], dim=-1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "\n",
    "    return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_OXFkVw1q4q"
   },
   "outputs": [],
   "source": [
    "class WorldModel(nn.Module):\n  def __init__(self, overshoot_d = 5):\n    super().__init__()\n    # self.encoder = Encoder() # obs -> e\n    self.rssm = RSSM() # h_old, s_old, obs, a_t -> m_pr, std_pr, m_po, std_po, h, s\n    self.decoder = Decoder() # h, s -> obs\n    self.reward = Reward() # h, s -> r\n    self.overshoot_d = overshoot_d\n\n\n  def forward(self, obs_seq, action_seq):\n    # obs_seq -> (step, B, 3, 64, 64)\n    # action_seq -> (step, B, action_dim)\n    T, B = obs_seq.shape[:2]\n    device = obs_seq.device\n\n    h = torch.zeros(B, 200).to(device)\n    s = torch.zeros(B, 30).to(device)\n\n    recon_img , pred_reward = [], []\n    prior_mean, prior_std = [], []\n    post_mean, post_std = [], []\n\n    h_all, s_all = [], []\n\n    for t in range(T):\n      # RSSM\n      # Fix 4: use previous action a_{t-1} — the transition h_t = f(h_{t-1}, s_{t-1}, a_{t-1})\n      prev_action = action_seq[t-1] if t > 0 else torch.zeros_like(action_seq[0])\n      p_m, p_s, q_m, q_s, h, s = self.rssm.obs_step(h, s, obs_seq[t], prev_action)\n\n      # Decode\n      recon_img_t = self.decoder(h, s)\n\n      # Reward\n      pred_reward_t = self.reward(h, s)\n\n      # COllecting distribution params\n      prior_mean.append(p_m)\n      prior_std.append(p_s)\n      post_mean.append(q_m)\n      post_std.append(q_s)\n\n      h_all.append(h)\n      s_all.append(s)\n\n      # Decode and Reward containers\n      recon_img.append(recon_img_t)\n      pred_reward.append(pred_reward_t)\n    \n    overshoot_kl_terms = []\n\n    for t in range(T - 1):\n      h_im = h_all[t].detach()\n      s_im = s_all[t].detach()\n\n      D = min(self.overshoot_d, T - 1 - t)\n\n      for d in range(1, D + 1):\n        im_m, im_s, h_im, s_im = self.rssm.imagine_step(\n          h_im, s_im, action_seq[t + d]\n        )\n        target_m = post_mean[t + d].detach()\n        target_s = post_std[t + d].detach()\n\n        kl = (\n          torch.log(im_s / target_s) +\n          (target_s**2 + (target_m - im_m)**2) / (2 * im_s**2)\n          - 0.5\n        )\n        overshoot_kl_terms.append(kl.sum(dim=-1).mean())\n    if overshoot_kl_terms:\n      overshoot_kl = torch.stack(overshoot_kl_terms).mean()\n    else:\n      overshoot_kl = torch.tensor(0.0, device=device)\n\n    return (torch.stack(recon_img),\n            torch.stack(pred_reward),\n            torch.stack(prior_mean),\n            torch.stack(prior_std),\n            torch.stack(post_mean),\n            torch.stack(post_std),\n            overshoot_kl\n    )\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "LXDG0kyKipGU",
    "outputId": "e865e1a8-263a-459a-ed24-78429aa369fd"
   },
   "outputs": [],
   "source": [
    "obs_seq = torch.rand(1, 1, 64, 64, 3)\n",
    "action_seq = torch.rand(1, 1, 4)\n",
    "print(obs_seq.shape, action_seq.shape)\n",
    "model = WorldModel()\n",
    "print(model(obs_seq, action_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wed4YnUYsnBd"
   },
   "source": [
    "$$D_{KL}(Q \\parallel P) = \\log \\left( \\frac{\\sigma_p}{\\sigma_q} \\right) + \\frac{\\sigma_q^2 + (\\mu_q - \\mu_p)^2}{2\\sigma_p^2} - \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRKVAAOdlAF6"
   },
   "outputs": [],
   "source": [
    "def calculate_loss(recon_img, img, reward, pred_reward, p_m, p_s, q_m, q_s,overshoot_kl, beta=0.1, beta_overshoot=0.1):\n",
    "  recon_loss = F.mse_loss(img, recon_img, reduction='none').sum(dim=[-1, -2, -3]).mean()\n",
    "  pred_loss = F.mse_loss(reward.unsqueeze(-1), pred_reward, reduction='none').mean()\n",
    "  kl_loss = torch.log(p_s/q_s) + ((q_s**2 + (q_m - p_m)**2)/(2*p_s**2)) - 0.5\n",
    "  kl_loss = kl_loss.sum(dim=-1).mean()\n",
    "  return recon_loss + pred_loss + beta * kl_loss  + beta_overshoot * overshoot_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRVaHjWL-A1S"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "  def __init__(self, capacity, obs_shape, action_dim):\n",
    "    self.capacity = capacity\n",
    "    self.idx = 0\n",
    "    self.is_full = False\n",
    "\n",
    "    self.obs_buffer = np.empty((capacity, *obs_shape), dtype=np.uint8)\n",
    "    self.action_buffer = np.empty((capacity, action_dim), dtype=np.float32)\n",
    "    self.reward_buffer = np.empty((capacity,), dtype=np.float32)\n",
    "    self.terminal_buffer = np.empty((capacity,), dtype=bool)\n",
    "\n",
    "  def add(self, obs, action, reward, terminal):\n",
    "    self.obs_buffer[self.idx] = obs\n",
    "    self.action_buffer[self.idx] = action\n",
    "    self.reward_buffer[self.idx] = reward\n",
    "    self.terminal_buffer[self.idx] = terminal\n",
    "\n",
    "\n",
    "    self.idx = (self.idx + 1) % self.capacity\n",
    "\n",
    "    if self.idx == 0:\n",
    "      self.is_full = True\n",
    "\n",
    "\n",
    "  def _is_valid(self, start, seq_len, current_capacity):\n",
    "      \"\"\"\n",
    "        Check if a sequence starting at 'start' is valid:\n",
    "        1. It doesn't fall off the end of the physical array.\n",
    "        2. It doesn't cross the circular 'write head' (self.idx).\n",
    "        3. It doesn't contain a terminal state in the middle (only allowed at the end).\n",
    "      \"\"\"\n",
    "      end = start + seq_len\n",
    "\n",
    "      # Bound Check\n",
    "      if end > current_capacity:\n",
    "        return False\n",
    "      # 2. Seam check: if buffer is full, sequence cannot cross self.idx\n",
    "      if self.is_full:\n",
    "        if start < self.idx < end:\n",
    "          return False\n",
    "      # 3. Episode boundary check: no terminals allowed in the middle of the clip\n",
    "      # We check from start to end-1 because the last frame can be terminal.\n",
    "      if self.terminal_buffer[start:end-1].any():\n",
    "        return False\n",
    "\n",
    "\n",
    "      return True\n",
    "\n",
    "\n",
    "\n",
    "  def sample(self, batch_size, seq_len, device):\n",
    "    current_capacity = self.capacity if self.is_full else self.idx\n",
    "\n",
    "    if current_capacity < seq_len:\n",
    "      return None\n",
    "\n",
    "    indices = []\n",
    "    attempts = 0\n",
    "    max_attempts = batch_size * 100\n",
    "    while len(indices) < batch_size and attempts < max_attempts:\n",
    "      start = np.random.randint(0, current_capacity)\n",
    "      if self._is_valid(start, seq_len, current_capacity):\n",
    "        indices.append(start)\n",
    "      attempts += 1\n",
    "\n",
    "    if len(indices) < batch_size:\n",
    "      return None # Not enough valid sequences found\n",
    "\n",
    "    obs_batch, action_batch, reward_batch, terminal_batch = [], [], [], []\n",
    "\n",
    "    for start in indices:\n",
    "      end = start + seq_len\n",
    "      obs_batch.append(self.obs_buffer[start:end])\n",
    "      action_batch.append(self.action_buffer[start:end])\n",
    "      reward_batch.append(self.reward_buffer[start:end])\n",
    "      terminal_batch.append(self.terminal_buffer[start:end])\n",
    "\n",
    "    obs_batch = np.stack(obs_batch)\n",
    "    action_batch = np.stack(action_batch)\n",
    "    reward_batch = np.stack(reward_batch)\n",
    "    terminal_batch = np.stack(terminal_batch)\n",
    "\n",
    "    return self._post_process(obs_batch, action_batch, reward_batch, terminal_batch, device)\n",
    "\n",
    "  def _post_process(self, obs, action, reward, terminal, device):\n",
    "    obs = torch.as_tensor(obs, device=device).float()\n",
    "    action = torch.as_tensor(action, device=device).float()\n",
    "    reward = torch.as_tensor(reward, device=device).float()\n",
    "    terminal = torch.as_tensor(terminal, device=device).float()\n",
    "\n",
    "    obs = obs/255.0\n",
    "\n",
    "    obs = obs.permute(1, 0, 2, 3, 4)\n",
    "    action = action.transpose(1, 0)\n",
    "    reward = reward.transpose(1, 0)\n",
    "    terminal = terminal.transpose(1, 0)\n",
    "\n",
    "    return obs, action, reward, terminal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsTFOx9yB-DF"
   },
   "outputs": [],
   "source": [
    "model = WorldModel()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "def train_step(model, optimizer, obs, action, reward, device):\n",
    "  recon_image, pred_reward, p_m, p_s, q_m, q_s, overshoot_kl = model(obs, action)\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss = calculate_loss(recon_image, obs, reward, pred_reward, p_m, p_s, q_m, q_s,overshoot_kl)\n",
    "  loss.backward()\n",
    "  clip_grad_norm_(\n",
    "      model.parameters(),\n",
    "      max_norm=100.0\n",
    "  )\n",
    "  optimizer.step()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEMPlanner:\n    def __init__(self, model, num_candidates=1000, top_k=100,  n_steps=12, iteration=10, action_dim=4):\n        self.model = model\n        self.num_candidates = num_candidates\n        self.top_k = top_k\n        self.n_steps = n_steps\n        self.iteration = iteration\n        self.action_dim = action_dim\n        self.epsillon = 0.1\n    \n    @torch.no_grad()\n    def plan(self, h, s):\n        if (h.ndim == 1):\n            h = h.unsqueeze(0)\n            s = s.unsqueeze(0)\n        device = h.device\n\n        mean = torch.zeros(self.n_steps, self.action_dim, device=device)\n        std = torch.ones(self.n_steps, self.action_dim, device=device)\n\n\n        for i in range(self.iteration):\n            noise = torch.randn(self.num_candidates, self.n_steps, self.action_dim, device=device)\n            actions = mean.unsqueeze(0) + std.unsqueeze(0)*noise\n\n            actions = actions.clamp(-2.0, 2.0)  # Fix 3: Pendulum-v1 torque range is [-2, 2]\n\n            # (1, 200) -> (num_candidates, 200)\n            h_im = h.expand(self.num_candidates, -1)\n            s_im = s.expand(self.num_candidates, -1)\n\n            total_rewards = torch.zeros(self.num_candidates, device=device)\n\n            for t in range(self.n_steps):\n                # actions[:, t] → (1000, 4) — all candidates' action at step t\n                _, _, h_im, s_im = self.model.rssm.imagine_step(h_im, s_im, actions[:, t])\n                reward = self.model.reward(h_im, s_im) # o -> (1000, 1)\n                total_rewards += reward.squeeze(-1)\n\n            top_indices = total_rewards.topk(self.top_k).indices\n\n            elite_actions = actions[top_indices]\n\n            mean = elite_actions.mean(dim=0) # (100, 12, 4) -> (12, 4)\n            std = elite_actions.std(dim=0) + self.epsillon\n\n        return mean[0] #(4,)\n\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_experience(env, model, planner, replay_buffer, num_episodes, device):\n     \"\"\"\n    Collects experience by interacting with the environment using the \n    CEM planner and stores it in the replay buffer.\n    \"\"\"\n    model.eval()\n\n    for _ in range(num_episodes):\n        obs, _ = env.reset()\n        done = False\n\n        h = torch.zeros(1, 200, device=device)\n        s = torch.zeros(1, 30, device=device)\n\n        # Fix 5: encode the first real observation before planning\n        with torch.no_grad():\n            init_obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0) / 255.0\n            dummy_a  = torch.zeros(1, planner.action_dim, device=device)\n            _, _, _, _, h, s = model.rssm.obs_step(h, s, init_obs[0], dummy_a)\n\n        while not done:\n            action = planner.plan(h, s)\n            action_np = action.cpu().numpy()\n\n            next_obs, reward, terminated, truncated, _ = env.step(action_np)\n            done = terminated or truncated\n\n            replay_buffer.add(obs, action_np, reward, done)\n\n            with torch.no_grad():\n                obs_tensor = torch.tensor(next_obs, dtype=torch.float32, device=device)\n                # Add Batch and Time dims\n                obs_tensor = obs_tensor.unsqueeze(0).unsqueeze(0) / 255.0\n                action_tensor = action.unsqueeze(0).unsqueeze(0)\n\n                # Use the RSSM observation step to update h and s based on the real observation\n                _, _, _, _, h, s = model.rssm.obs_step(h,s, obs_tensor[0], action_tensor[0])\n            \n            obs = next_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_planner(env, model, optimizer, planner, replay_buffer, config):\n",
    "    device = config.device\n",
    "\n",
    "    # 1. Seed the Replay Buffer with random experience\n",
    "    # Initially, the model knows nothing, so planning is useless. \n",
    "    # We collect uniform random actions to get a starting dataset.\n",
    "\n",
    "\n",
    "    print(\"Collecting initial random seed experience...\")\n",
    "\n",
    "    for _ in range(config.seed_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            replay_buffer.add(obs, action, reward, done)\n",
    "            obs = next_obs\n",
    "\n",
    "    # 2. Main Training Iterations\n",
    "    for iteration in range(config.total_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{config.total_iterations}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for _ in range(config.train_steps_per_iteration):\n",
    "            batch = replay_buffer.sample(config.batch_size, config.seq_len, device)\n",
    "\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            obs_batch, action_batch, reward_batch, terminal_batch = batch\n",
    "\n",
    "            loss = train_step(model, optimizer, obs_batch, action_batch, reward_batch, device)\n",
    "\n",
    "            total_loss += loss\n",
    "        print(f\"Average Loss: {total_loss / config.train_steps_per_iteration:.4f}\")\n",
    "\n",
    "        collect_experience(env, model, planner, replay_buffer, config.collect_episodes, device)\n",
    "\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n    def __init__(self):\n        # Hardware\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Training loop parameters\n        self.seed_episodes = 5              # Initial episodes with random actions to seed the buffer\n        self.total_iterations = 100         # Total number of iterations (train + collect cycles)\n        self.train_steps_per_iteration = 1000 # Number of gradient updates per iteration\n        self.collect_episodes = 1           # Number of new episodes to collect after training\n        \n        # Replay Buffer parameters\n        self.batch_size = 50                # Number of sequences in a batch\n        self.seq_len = 50                   # Length of each sequence chunk sampled for BPTT\n        self.capacity = 100000              # Total transitions to store in the replay buffer\n        \n        # Environment parameters\n        self.obs_shape = (64, 64, 3)        # Shape of the environment observations\n        self.action_dim = 1                 # Fix 2: Pendulum-v1 has a 1-D continuous action\n        \n# Usage:\nconfig = Config()\n# train_planner(env, model, optimizer, planner, replay_buffer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, render_size=64):\n",
    "        super().__init__(env)\n",
    "        self.render_size = render_size\n",
    "\n",
    "        # Override the observation space to be an image\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255,\n",
    "            shape=(render_size, render_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "    def _get_pixels(self):\n",
    "        # Render the environment as an RGB array\n",
    "        img = self.env.render()\n",
    "            \n",
    "        # Resize to exactly 64x64\n",
    "        if img.shape[:2] != (self.render_size, self.render_size):\n",
    "            img = cv2.resize(img, (self.render_size, self.render_size), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "        return img\n",
    "    def reset(self, **kwargs):\n",
    "        _ = self.env.reset(**kwargs)\n",
    "        return self._get_pixels(), {}\n",
    "    def step(self, action):\n",
    "        _, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return self._get_pixels(), reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_planer(env, model, planner, num_episode, device):\n    \"\"\"\n    Evaluates the World Model's performance in the environment.\n    \"\"\"\n    model.eval()\n    total_rewards = []\n\n    for ep in range(num_episode):\n        obs, _ = env.reset()\n        done = False\n        episode_reward = 0\n\n        h = torch.zeros(1, 200, device=device)\n        s = torch.zeros(1, 30, device=device)\n\n        # Fix 5: encode the first real observation before planning\n        with torch.no_grad():\n            init_obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0) / 255.0\n            dummy_a  = torch.zeros(1, planner.action_dim, device=device)\n            _, _, _, _, h, s = model.rssm.obs_step(h, s, init_obs[0], dummy_a)\n\n        while not done:\n            with torch.no_grad():\n                action = planner.plan(h, s)\n                action_np = action.cpu().numpy()\n                next_obs, reward, terminated, truncated, _ = env.step(action_np)\n\n                done = terminated or truncated\n                episode_reward += reward\n                obs_tensor = torch.tensor(next_obs, dtype=torch.float32, device=device)\n\n                obs_tensor = obs_tensor.unsqueeze(0).unsqueeze(0) / 255.0\n                action_tensor = action.unsqueeze(0).unsqueeze(0)\n                _, _, _, _, h, s = model.rssm.obs_step(h, s, obs_tensor[0], action_tensor[0])\n        total_rewards.append(episode_reward)\n        print(f\"Eval Episode {ep+1}: Reward = {episode_reward:.2f}\")\n    avg_reward = np.mean(total_rewards)\n    print(f\"Average Evaluation Reward: {avg_reward:.2f}\")\n    return avg_reward\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = gym.make('Pendulum-v1', render_mode='rgb_array')\n\nenv = PixelWrapper(base_env, render_size=64)\n\nconfig = Config()\ndevice = config.device\n\nmodel = WorldModel().to(device)\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\n\nplanner = CEMPlanner(model, action_dim=config.action_dim)  # Fix 1: corrected typo CEMPlanner; pass action_dim from config\nreplay_buffer = ReplayBuffer(config.capacity, config.obs_shape, config.action_dim)\nprint(\"Starting PlaNet Training...\")\ntrain_planner(env, model, optimizer, planner, replay_buffer, config)\nevaluate_planer(  # Fix 1: corrected function name to match definitionenv, model, planner, 5, device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}