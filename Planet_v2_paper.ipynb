{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "G4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gqS53zsIi286"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3475LbDXeCPk",
        "outputId": "adb12e4e-49ab-4c82-af77-bf923295a729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.12/dist-packages (0.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imageio) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install imageio imageio-ffmpeg matplotlib gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab — MuJoCo needs a virtual display for headless rendering\n",
        "!apt-get install -y xvfb\n",
        "!pip install mujoco gymnasium[mujoco] pyvirtualdisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "Display(visible=0, size=(1400, 900)).start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRfSU72aQNbh",
        "outputId": "b99fcc58-d966-4f63-df9a-a976b8c64cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.16).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Collecting mujoco\n",
            "  Downloading mujoco-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.13.0)\n",
            "Collecting glfw (from mujoco)\n",
            "  Downloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.py39.py310.py311.py312.py313.py314-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mujoco) (2.0.2)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco) (3.1.10)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (2.37.2)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]) (26.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.3.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (3.23.0)\n",
            "Downloading mujoco-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Downloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.py39.py310.py311.py312.py313.py314-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyvirtualdisplay, glfw, mujoco\n",
            "Successfully installed glfw-2.10.0 mujoco-3.5.0 pyvirtualdisplay-3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x782fc3332750>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "#  Paper: \"Learning Latent Dynamics for Planning from Pixels\" (Hafner et al.)\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import matplotlib; matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio"
      ],
      "metadata": {
        "id": "tqNlYcvxeQVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "#  CONFIG  — single source of truth for every dimension in the model\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class Config:\n",
        "    # ── Environment ──────────────────────────────────────────────────────────\n",
        "    env_name   = 'HalfCheetah-v4'\n",
        "    #  Common action_dim values:\n",
        "    #    HalfCheetah-v4 → 6 | Hopper-v4 → 3 | Walker2d-v4 → 6 | Ant-v4 → 8\n",
        "    action_dim = 6\n",
        "\n",
        "    # ── Model dimensions ─────────────────────────────────────────────────────\n",
        "    # Bumped from Pendulum defaults (hidden=200, state=30) because MuJoCo tasks\n",
        "    # have richer dynamics that require larger representational capacity.\n",
        "    hidden_size = 400   # GRU / deterministic state h_t\n",
        "    state_size  = 50    # stochastic latent state s_t\n",
        "    enc_dim     = 1024  # Encoder output (fixed by the conv stack — do not change\n",
        "                        # without also changing the conv channel widths)\n",
        "\n",
        "    # ── Training ─────────────────────────────────────────────────────────────\n",
        "    total_iterations     = 120  # MuJoCo tasks need more iterations than Pendulum\n",
        "    train_steps_per_iter = 100\n",
        "    batch_size           = 50\n",
        "    seq_len              = 50    # longer sequences to capture locomotion cycles\n",
        "    lr                   = 6e-4\n",
        "    seed_episodes        = 10   # more diverse seeds for harder tasks\n",
        "    collect_episodes     = 1\n",
        "    overshoot_d          = 5\n",
        "\n",
        "    # ── Google Drive paths  (keyed by env_name → no checkpoint conflicts) ────\n",
        "    drive_base     = '/content/drive/MyDrive/PlaNet'\n",
        "    episode_dir    = f'{drive_base}/{env_name}/episodes'\n",
        "    checkpoint_dir = f'{drive_base}/{env_name}/checkpoints'\n",
        "    viz_dir        = f'{drive_base}/{env_name}/visualizations'\n",
        "    checkpoint_every  = 10\n",
        "    keep_checkpoints  = 5\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "taj3ZX2Bfc_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "#  1.  ENCODER   (B, 64, 64, 3)  →  (B, enc_dim)\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Four strided convolutions halve spatial resolution each step:\n",
        "        64×64 → 32×32 → 16×16 → 8×8 → 4×4  (each: kernel=4, stride=2, pad=1)\n",
        "    A linear layer then projects the 256×4×4 = 4096 features → enc_dim.\n",
        "\n",
        "    enc_dim (default 1024) is the interface between the encoder and the\n",
        "    posterior.  Changing it would require updating the Posterior linear as well.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config, in_channels: int = 3):\n",
        "        super().__init__()\n",
        "        self.cv1 = nn.Conv2d(in_channels, 32,  4, 2, 1)   # → (B, 32,  32, 32)\n",
        "        self.cv2 = nn.Conv2d(32,          64,  4, 2, 1)   # → (B, 64,  16, 16)\n",
        "        self.cv3 = nn.Conv2d(64,          128, 4, 2, 1)   # → (B, 128,  8,  8)\n",
        "        self.cv4 = nn.Conv2d(128,         256, 4, 2, 1)   # → (B, 256,  4,  4)\n",
        "        self.fc  = nn.Linear(256 * 4 * 4, cfg.enc_dim)    # → (B, enc_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, H, W, C)  —  permute to (B, C, H, W) for Conv2d\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = F.relu(self.cv1(x))\n",
        "        x = F.relu(self.cv2(x))\n",
        "        x = F.relu(self.cv3(x))\n",
        "        x = F.relu(self.cv4(x))\n",
        "        return self.fc(x.flatten(1))   # (B, enc_dim)"
      ],
      "metadata": {
        "id": "k3NblrwzeW5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "#  2.  GRU  — deterministic transition\n",
        "#      h_t  =  GRUCell( [s_{t-1}, a_{t-1}],  h_{t-1} )\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class GRU(nn.Module):\n",
        "    \"\"\"\n",
        "    Maintains the deterministic hidden state h_t which carries long-range\n",
        "    temporal information across the sequence.\n",
        "\n",
        "    Input to GRUCell: concat(s_{t-1}, a_{t-1})\n",
        "        shape: (B, state_size + action_dim)\n",
        "    Output h_t: (B, hidden_size)\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.cell = nn.GRUCell(cfg.state_size + cfg.action_dim, cfg.hidden_size)\n",
        "\n",
        "    def forward(self, s_t, a_t, h_old):\n",
        "        # s_t: (B, state_size)  a_t: (B, action_dim)  h_old: (B, hidden_size)\n",
        "        return self.cell(torch.cat([s_t, a_t], dim=-1), h_old)  # (B, hidden_size)"
      ],
      "metadata": {
        "id": "7Mi5jsPNea_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "#  4.  POSTERIOR   q(s_t | h_t, e_t)\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class Posterior(nn.Module):\n",
        "    \"\"\"\n",
        "    Refines the prior's state estimate using the real encoded observation.\n",
        "    Called only during training — the encoder is never run at planning time.\n",
        "\n",
        "    Input: concat(e_t, h_t)  →  (B, enc_dim + hidden_size)\n",
        "    Output: mean, std  each (B, state_size)\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.fc     = nn.Linear(cfg.enc_dim + cfg.hidden_size, 256)\n",
        "        self.fc_mu  = nn.Linear(256, cfg.state_size)\n",
        "        self.fc_std = nn.Linear(256, cfg.state_size)\n",
        "\n",
        "    def forward(self, e_t, h_t):\n",
        "        x   = F.relu(self.fc(torch.cat([e_t, h_t], dim=-1)))\n",
        "        mu  = self.fc_mu(x)\n",
        "        std = F.softplus(self.fc_std(x)) + 0.1\n",
        "        return mu, std"
      ],
      "metadata": {
        "id": "JrgiOuVYefwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "#  3.  PRIOR   p(s_t | h_t)\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class Prior(nn.Module):\n",
        "    \"\"\"\n",
        "    Predicts the stochastic state from the deterministic state alone.\n",
        "    Used at planning time (no encoder available) and for the KL loss term.\n",
        "\n",
        "    h_t (B, hidden_size)  →  mean, std  each (B, state_size)\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.fc     = nn.Linear(cfg.hidden_size, 256)\n",
        "        self.fc_mu  = nn.Linear(256, cfg.state_size)\n",
        "        self.fc_std = nn.Linear(256, cfg.state_size)\n",
        "\n",
        "    def forward(self, h):\n",
        "        x   = F.relu(self.fc(h))\n",
        "        mu  = self.fc_mu(x)\n",
        "        std = F.softplus(self.fc_std(x)) + 0.1   # +0.1 floor prevents collapse\n",
        "        return mu, std\n"
      ],
      "metadata": {
        "id": "lSmUNT-BejEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "#  5.  RSSM  — Recurrent State Space Model\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class RSSM(nn.Module):\n",
        "    \"\"\"\n",
        "    Core PlaNet module combining Encoder + GRU + Prior + Posterior.\n",
        "\n",
        "    Two operating modes:\n",
        "      obs_step     — training: uses real obs via encoder → posterior\n",
        "      imagine_step — planning: prior only, encoder never called\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.encoder   = Encoder(cfg)\n",
        "        self.gru       = GRU(cfg)\n",
        "        self.prior     = Prior(cfg)\n",
        "        self.posterior = Posterior(cfg)\n",
        "\n",
        "    def obs_step(self, h_old, s_old, obs, a_prev):\n",
        "        \"\"\"\n",
        "        Training step — uses the real observation via the posterior.\n",
        "\n",
        "        Args:\n",
        "            h_old : (B, hidden_size)  previous deterministic state\n",
        "            s_old : (B, state_size)   previous stochastic  state\n",
        "            obs   : (B, 64, 64, 3)   current pixel obs, normalised [0, 1]\n",
        "            a_prev: (B, action_dim)   *previous* action a_{t-1}\n",
        "\n",
        "        Returns:\n",
        "            p_m, p_s  : prior  params   (B, state_size) each\n",
        "            q_m, q_s  : posterior params\n",
        "            h, s      : new (h_t, s_t)\n",
        "        \"\"\"\n",
        "        h          = self.gru(s_old, a_prev, h_old)          # (B, hidden_size)\n",
        "        e          = self.encoder(obs)                         # (B, enc_dim)\n",
        "        p_m, p_s   = self.prior(h)                            # (B, state_size) each\n",
        "        q_m, q_s   = self.posterior(e, h)\n",
        "        # Reparameterisation: s = mu + std * eps,  eps ~ N(0,I)\n",
        "        # This lets gradients flow through the sampling op to both networks.\n",
        "        s          = q_m + q_s * torch.randn_like(q_m)        # (B, state_size)\n",
        "        return p_m, p_s, q_m, q_s, h, s\n",
        "\n",
        "    def imagine_step(self, h_old, s_old, a_t):\n",
        "        \"\"\"\n",
        "        Dream/planning step — prior only, NO encoder called.\n",
        "\n",
        "        Returns: p_m, p_s, h, s\n",
        "        \"\"\"\n",
        "        h          = self.gru(s_old, a_t, h_old)\n",
        "        p_m, p_s   = self.prior(h)\n",
        "        s          = p_m + p_s * torch.randn_like(p_m)\n",
        "        return p_m, p_s, h, s"
      ],
      "metadata": {
        "id": "UAqTXo12enS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "#  6.  DECODER   (h, s)  →  (B, 64, 64, 3)\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Mirrors the Encoder using transposed convolutions.\n",
        "\n",
        "    concat(h, s)  →  linear  →  reshape  →  4×ConvTranspose2d  →  pixels\n",
        "\n",
        "    (B, hidden+state)\n",
        "      → (B, 4096)   linear + relu\n",
        "      → (B,256,4,4) reshape\n",
        "      → (B,128,8,8)  ConvTranspose2d\n",
        "      → (B,64,16,16)\n",
        "      → (B,32,32,32)\n",
        "      → (B,3,64,64)  sigmoid → [0,1]\n",
        "      → (B,64,64,3)  permute back to HWC\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.fc   = nn.Linear(cfg.state_size + cfg.hidden_size, 4096)\n",
        "        self.dec1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
        "        self.dec2 = nn.ConvTranspose2d(128, 64,  4, 2, 1)\n",
        "        self.dec3 = nn.ConvTranspose2d(64,  32,  4, 2, 1)\n",
        "        self.dec4 = nn.ConvTranspose2d(32,  3,   4, 2, 1)\n",
        "\n",
        "    def forward(self, h, s):\n",
        "        x = F.relu(self.fc(torch.cat([h, s], dim=-1))).reshape(-1, 256, 4, 4)\n",
        "        x = F.relu(self.dec1(x))\n",
        "        x = F.relu(self.dec2(x))\n",
        "        x = F.relu(self.dec3(x))\n",
        "        return torch.sigmoid(self.dec4(x)).permute(0, 2, 3, 1)  # (B,64,64,3)\n"
      ],
      "metadata": {
        "id": "T8bMft11eqPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "#  7.  REWARD HEAD   (h, s)  →  (B, 1)\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class Reward(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP that predicts the scalar reward from the latent state.\n",
        "    Training this head keeps the latent space task-relevant, not just visually\n",
        "    faithful — crucial for the CEM planner to find high-reward trajectories.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config, hidden_dim: int = 400):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.state_size + cfg.hidden_size, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, h, s):\n",
        "        x = F.relu(self.fc1(torch.cat([s, h], dim=-1)))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)   # (B, 1)"
      ],
      "metadata": {
        "id": "MGwMO9x1euL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "#  8.  WORLD MODEL\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class WorldModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Ties RSSM + Decoder + Reward together and runs them over a (T, B) sequence.\n",
        "\n",
        "    Also computes the latent overshooting KL (Section 3.3 of the paper):\n",
        "    for each timestep t, we imagine D steps into the future from the posterior\n",
        "    state at t and compare those imagined prior distributions to the actual\n",
        "    posterior distributions at t+1 … t+D.  This forces the prior to be\n",
        "    accurate not just 1-step ahead (which is all the standard KL covers) but\n",
        "    across the full planning horizon — making the imagined rollouts reliable\n",
        "    for CEM.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: Config):\n",
        "        super().__init__()\n",
        "        self.rssm        = RSSM(cfg)\n",
        "        self.decoder     = Decoder(cfg)\n",
        "        self.reward      = Reward(cfg)\n",
        "        self.overshoot_d = cfg.overshoot_d\n",
        "        self.cfg         = cfg   # store for zero-init in forward\n",
        "\n",
        "    def forward(self, obs_seq, action_seq):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            obs_seq    : (T, B, 64, 64, 3)  float32 in [0, 1]\n",
        "            action_seq : (T, B, action_dim)\n",
        "\n",
        "        Returns:\n",
        "            recon_img     : (T, B, 64, 64, 3)\n",
        "            pred_reward   : (T, B, 1)\n",
        "            prior_mean/std: (T, B, state_size) each\n",
        "            post_mean/std : (T, B, state_size) each\n",
        "            overshoot_kl  : scalar\n",
        "        \"\"\"\n",
        "        T, B   = obs_seq.shape[:2]\n",
        "        device = obs_seq.device\n",
        "        cfg    = self.cfg\n",
        "\n",
        "        # Initialise hidden states to zero — no prior episode context\n",
        "        h = torch.zeros(B, cfg.hidden_size, device=device)\n",
        "        s = torch.zeros(B, cfg.state_size,  device=device)\n",
        "\n",
        "        recon_img, pred_reward = [], []\n",
        "        prior_mean, prior_std  = [], []\n",
        "        post_mean,  post_std   = [], []\n",
        "        h_all, s_all           = [], []\n",
        "\n",
        "        for t in range(T):\n",
        "            # Use a_{t-1} to avoid causal leakage (current action unknown)\n",
        "            prev_a = action_seq[t-1] if t > 0 else torch.zeros_like(action_seq[0])\n",
        "            p_m, p_s, q_m, q_s, h, s = self.rssm.obs_step(h, s, obs_seq[t], prev_a)\n",
        "\n",
        "            recon_img.append(self.decoder(h, s))    # (B, 64, 64, 3)\n",
        "            pred_reward.append(self.reward(h, s))   # (B, 1)\n",
        "            prior_mean.append(p_m);  prior_std.append(p_s)\n",
        "            post_mean.append(q_m);   post_std.append(q_s)\n",
        "            h_all.append(h);         s_all.append(s)\n",
        "\n",
        "        # ── Latent overshooting ───────────────────────────────────────────────\n",
        "        os_kl = []\n",
        "        for t in range(T - 1):\n",
        "            # Detach so overshooting does not backprop through the main sequence\n",
        "            hi, si = h_all[t].detach(), s_all[t].detach()\n",
        "            D = min(self.overshoot_d, T - 1 - t)\n",
        "            for d in range(1, D + 1):\n",
        "                im_m, im_s, hi, si = self.rssm.imagine_step(hi, si, action_seq[t+d-1])\n",
        "                tgt_m = post_mean[t+d].detach()\n",
        "                tgt_s = post_std[t+d].detach()\n",
        "                # Closed-form KL divergence between two diagonal Gaussians\n",
        "                kl = (torch.log(im_s / tgt_s)\n",
        "                      + (tgt_s**2 + (tgt_m - im_m)**2) / (2 * im_s**2) - 0.5)\n",
        "                os_kl.append(kl.sum(dim=-1).mean())\n",
        "\n",
        "        overshoot_kl = (torch.stack(os_kl).mean()\n",
        "                        if os_kl else torch.tensor(0.0, device=device))\n",
        "\n",
        "        return (torch.stack(recon_img), torch.stack(pred_reward),\n",
        "                torch.stack(prior_mean), torch.stack(prior_std),\n",
        "                torch.stack(post_mean),  torch.stack(post_std),\n",
        "                overshoot_kl)\n"
      ],
      "metadata": {
        "id": "mfk10U5Nex7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "#  9.  LOSS\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "def calculate_loss(recon_img, img, reward, pred_reward,\n",
        "                   p_m, p_s, q_m, q_s, overshoot_kl,\n",
        "                   beta=0.1, beta_overshoot=0.1):\n",
        "    \"\"\"\n",
        "    Three loss terms:\n",
        "      1. Recon loss — MSE(real_pixels, decoded_pixels), summed over spatial dims\n",
        "      2. Reward loss — MSE(real_reward, predicted_reward)\n",
        "      3. KL loss    — D_KL(posterior || prior), weighted by beta\n",
        "         + overshooting KL weighted by beta_overshoot\n",
        "    \"\"\"\n",
        "    recon_loss = F.mse_loss(img, recon_img, reduction='none').sum(dim=[-1,-2,-3]).mean()\n",
        "    pred_loss  = F.mse_loss(reward.unsqueeze(-1), pred_reward).mean()\n",
        "    kl_loss    = (torch.log(p_s / q_s)\n",
        "                  + (q_s**2 + (q_m - p_m)**2) / (2 * p_s**2) - 0.5).sum(-1)\n",
        "    kl_loss = torch.clamp(kl_loss, min=3.0).mean()\n",
        "    # beta = min(0.1, 0.1 * (iteration / 30))   # ramp 0→0.1 over 30 iters\n",
        "    return recon_loss + pred_loss + beta * kl_loss + beta_overshoot * overshoot_kl"
      ],
      "metadata": {
        "id": "9uBqwED3ezQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 10a. EPISODE STORAGE  [NEW – replaces in-memory circular buffer]\n",
        "#\n",
        "# WHAT:  EpisodeStorage saves each collected episode as a compressed .npz file\n",
        "#        directly to Google Drive (or any folder you point it at).\n",
        "#\n",
        "# WHY:   Colab VMs are ephemeral – their RAM and /tmp vanish when the session\n",
        "#        ends.  By writing raw episode files to Drive we get:\n",
        "#          • Free persistent storage that survives runtime restarts.\n",
        "#          • We can inspect / visualise individual episodes in Drive.\n",
        "#          • The dataset can grow across many sessions without re-collecting.\n",
        "#\n",
        "# HOW:   On every call to add_episode() we:\n",
        "#          1. Stack the transition lists into numpy arrays.\n",
        "#          2. Save them as episode_XXXXXX.npz in the given directory.\n",
        "#          3. Append the filename to a manifest list so EpisodeDataset can\n",
        "#             index them without scanning the folder on every access.\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 11.  EPISODE STORAGE\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class EpisodeStorage:\n",
        "    \"\"\"Saves each episode as a compressed .npz on Drive; reloads on restart.\"\"\"\n",
        "    def __init__(self, episode_dir: str):\n",
        "        self.episode_dir   = pathlib.Path(episode_dir)\n",
        "        self.episode_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.episode_paths = sorted(self.episode_dir.glob('episode_*.npz'))\n",
        "        print(f\"[Storage] {len(self.episode_paths)} existing episodes in {episode_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.episode_paths)\n",
        "\n",
        "    def add_episode(self, obs_list, action_list, reward_list, terminal_list):\n",
        "        idx  = len(self.episode_paths)\n",
        "        path = self.episode_dir / f'episode_{idx:06d}.npz'\n",
        "        np.savez_compressed(path,\n",
        "            obs      = np.array(obs_list,      dtype=np.uint8),\n",
        "            action   = np.array(action_list,   dtype=np.float32),\n",
        "            reward   = np.array(reward_list,   dtype=np.float32),\n",
        "            terminal = np.array(terminal_list, dtype=bool))\n",
        "        self.episode_paths.append(path)\n",
        "\n"
      ],
      "metadata": {
        "id": "EZvuXZ8De9cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 10b. EPISODE DATASET  [NEW]\n",
        "#\n",
        "# WHAT:  A PyTorch Dataset that wraps the saved .npz files on Drive and\n",
        "#        samples random fixed-length subsequences for BPTT training.\n",
        "#\n",
        "# WHY:   Using Dataset + DataLoader gives us:\n",
        "#          • Multi-worker prefetching (num_workers > 0) so GPU never starves.\n",
        "#          • Standard PyTorch shuffle / sampler interface.\n",
        "#          • Lazy loading: only the sequences actually needed are read off disk.\n",
        "#\n",
        "# HOW:   __getitem__ picks a random start index within a random episode,\n",
        "#        reads just that slice from the .npz, normalises obs to [0,1] float\n",
        "#        and returns (obs, action, reward, terminal) tensors of length seq_len.\n",
        "#        collate_episode_batch() transposes the resulting batch from\n",
        "#        (B, T, …) → (T, B, …) so it matches WorldModel.forward.\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 12.  EPISODE DATASET\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class EpisodeDataset(Dataset):\n",
        "    def __init__(self, storage: EpisodeStorage, seq_len: int, virtual_len: int):\n",
        "        self.seq_len     = seq_len\n",
        "        self.virtual_len = virtual_len\n",
        "\n",
        "        # ── Load ALL valid episodes into RAM (once, at construction time) ────\n",
        "        #       so __getitem__ only does an in-memory slice + dtype cast.\n",
        "        self.episodes = []   # list of {obs, action, reward, terminal}\n",
        "        skipped = 0\n",
        "        for path in storage.episode_paths:\n",
        "            with np.load(path) as ep:\n",
        "                T = len(ep['reward'])\n",
        "                if T >= seq_len:\n",
        "                    # Copy arrays out of the NpzFile so the file can close\n",
        "                    self.episodes.append({\n",
        "                        'obs'     : ep['obs'].copy(),       # (T, 64, 64, 3) uint8\n",
        "                        'action'  : ep['action'].copy(),    # (T, action_dim)\n",
        "                        'reward'  : ep['reward'].copy(),    # (T,)\n",
        "                        'terminal': ep['terminal'].copy(),  # (T,)\n",
        "                    })\n",
        "                else:\n",
        "                    skipped += 1\n",
        "\n",
        "        if skipped:\n",
        "            print(f\"[EpisodeDataset] Skipped {skipped} short episodes \"\n",
        "                  f\"(< {seq_len} frames).\")\n",
        "        if len(self.episodes) == 0:\n",
        "            raise RuntimeError(\n",
        "                f\"No episodes with >= {seq_len} frames found. \"\n",
        "                f\"Collect more data or reduce seq_len.\"\n",
        "            )\n",
        "        total_mb = sum(ep['obs'].nbytes for ep in self.episodes) / 1e6\n",
        "        print(f\"[EpisodeDataset] Cached {len(self.episodes)} episodes \"\n",
        "              f\"({total_mb:.1f} MB) | virtual_len={virtual_len}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.virtual_len\n",
        "\n",
        "    def __getitem__(self, _idx):\n",
        "        # ── Recency-weighted episode selection ──────────────────\n",
        "        n       = len(self.episodes)\n",
        "        weights = np.linspace(0.5, 1.0, n)\n",
        "        weights = weights / weights.sum()                              # normalise → probabilities\n",
        "        idx     = np.random.choice(n, p=weights)                      # weighted draw\n",
        "        ep      = self.episodes[idx]\n",
        "\n",
        "        T     = len(ep['reward'])\n",
        "        start = np.random.randint(0, T - self.seq_len + 1)\n",
        "        end   = start + self.seq_len\n",
        "\n",
        "        obs      = ep['obs'][start:end].astype(np.float32) / 255.0  # (seq_len, 64, 64, 3)\n",
        "        action   = ep['action'][start:end]                           # (seq_len, action_dim)\n",
        "        reward   = ep['reward'][start:end]                           # (seq_len,)\n",
        "        terminal = ep['terminal'][start:end].astype(np.float32)     # (seq_len,)\n",
        "\n",
        "        return (\n",
        "            torch.from_numpy(obs),\n",
        "            torch.from_numpy(action),\n",
        "            torch.from_numpy(reward),\n",
        "            torch.from_numpy(terminal),\n",
        "        )\n",
        "\n",
        "    def add_episode(self, path: 'pathlib.Path') -> None:\n",
        "        \"\"\"\n",
        "        Append a newly collected episode to the in-RAM cache.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : pathlib.Path  Path to the newly written .npz file on Drive.\n",
        "        \"\"\"\n",
        "        with np.load(path) as ep:\n",
        "            T = len(ep['reward'])\n",
        "            if T >= self.seq_len:\n",
        "                self.episodes.append({k: np.array(ep[k]) for k in ep})\n",
        "            else:\n",
        "                print(f\"[Dataset] Skipped short episode ({T} < {self.seq_len} frames)\")\n"
      ],
      "metadata": {
        "id": "-LWO6eWtfA50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_episode_batch(batch):\n",
        "    \"\"\"\n",
        "    WHAT: Custom collate for DataLoader.\n",
        "    WHY:  Default collate stacks to (B, T, …).  WorldModel.forward expects\n",
        "          (T, B, …), so we transpose here once instead of in every train step.\n",
        "    HOW:  torch.stack along dim=0 then permute the time and batch dims.\n",
        "    \"\"\"\n",
        "    obs, action, reward, terminal = zip(*batch)\n",
        "    return (torch.stack(obs).permute(1,0,2,3,4),\n",
        "            torch.stack(action).permute(1,0,2),\n",
        "            torch.stack(reward).permute(1,0),\n",
        "            torch.stack(terminal).permute(1,0))"
      ],
      "metadata": {
        "id": "JTS1q-nrfDyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 14.  TRAIN STEP\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "def train_step(model, optimizer, obs, action, reward, device):\n",
        "    \"\"\"One forward + backward pass.  Clips gradient norm to prevent explosion.\"\"\"\n",
        "    recon_image, pred_reward, p_m, p_s, q_m, q_s, os_kl = model(obs, action)\n",
        "    loss = calculate_loss(recon_image, obs, reward, pred_reward,\n",
        "                          p_m, p_s, q_m, q_s, os_kl)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    clip_grad_norm_(model.parameters(), max_norm=100.0)\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "Vo-29A5sfOIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 13.  CEM PLANNER\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class CEMPlanner:\n",
        "    \"\"\"\n",
        "    Cross-Entropy Method planning entirely in latent space.\n",
        "    Samples num_candidates action sequences, rolls them out via imagine_step,\n",
        "    scores by summed predicted reward, refits a Gaussian to the top_k elite\n",
        "    sequences, repeats n_iter times, returns the first action.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: WorldModel, cfg: Config,\n",
        "                 num_candidates=1000, top_k=100, n_steps=12, n_iter=10):\n",
        "        self.model          = model\n",
        "        self.cfg            = cfg\n",
        "        self.num_candidates = num_candidates\n",
        "        self.top_k          = top_k\n",
        "        self.n_steps        = n_steps\n",
        "        self.n_iter         = n_iter\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def plan(self, h, s, device):\n",
        "        # Gaussian belief over action sequences: (n_steps, action_dim)\n",
        "        mu  = torch.zeros(self.n_steps, self.cfg.action_dim, device=device)\n",
        "        std = torch.ones_like(mu)\n",
        "\n",
        "        for _ in range(self.n_iter):\n",
        "            # Sample (num_candidates, n_steps, action_dim)\n",
        "            acts = (mu + std * torch.randn(\n",
        "                self.num_candidates, self.n_steps,\n",
        "                self.cfg.action_dim, device=device)).clamp(-1, 1)\n",
        "\n",
        "            H = h.expand(self.num_candidates, -1)   # (K, hidden_size)\n",
        "            S = s.expand(self.num_candidates, -1)   # (K, state_size)\n",
        "            G = torch.zeros(self.num_candidates, device=device)\n",
        "\n",
        "            for t in range(self.n_steps):\n",
        "                _, _, H, S = self.model.rssm.imagine_step(H, S, acts[:, t])\n",
        "                G         += self.model.reward(H, S).squeeze(-1)\n",
        "\n",
        "            # Refit to top_k elite sequences\n",
        "            top = G.topk(self.top_k).indices\n",
        "            mu  = acts[top].mean(0)\n",
        "            std = acts[top].std(0).clamp(min=1e-4)\n",
        "\n",
        "        return mu[0]   # first action from the best sequence"
      ],
      "metadata": {
        "id": "DxWAy7qSfTJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 10.  PIXEL WRAPPER\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "class PixelWrapper(gym.Wrapper):\n",
        "    \"\"\"Replaces the env's proprioceptive obs with rendered 64×64 RGB pixels.\"\"\"\n",
        "    def __init__(self, env, render_size: int = 64):\n",
        "        super().__init__(env)\n",
        "        self.render_size = render_size\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=255, shape=(render_size, render_size, 3), dtype=np.uint8)\n",
        "\n",
        "    def _get_pixels(self):\n",
        "        img = self.env.render()\n",
        "        if isinstance(img, list):\n",
        "            img = img[0]\n",
        "        img = np.asarray(img)\n",
        "        if img.shape[:2] != (self.render_size, self.render_size):\n",
        "            img = cv2.resize(img, (self.render_size, self.render_size),\n",
        "                             interpolation=cv2.INTER_AREA)\n",
        "        return img   # (H, W, 3) uint8\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        _ = self.env.reset(**kwargs)\n",
        "        return self._get_pixels(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        _, reward, terminated, truncated, info = self.env.step(action)\n",
        "        return self._get_pixels(), reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "r9YICm5xfZS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 16.  EXPERIENCE COLLECTION\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "def collect_experience(env, model: WorldModel, planner: CEMPlanner,\n",
        "                       storage: EpisodeStorage, n_episodes: int,\n",
        "                       config: Config):\n",
        "    \"\"\"Run n CEM episodes and persist them to storage.\"\"\"\n",
        "    model.eval()\n",
        "    cfg = config\n",
        "    for _ in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        done   = False\n",
        "        h = torch.zeros(1, cfg.hidden_size, device=cfg.device)\n",
        "        s = torch.zeros(1, cfg.state_size,  device=cfg.device)\n",
        "        prev_a = torch.zeros(1, cfg.action_dim, device=cfg.device)\n",
        "        obs_l, act_l, rew_l, term_l = [], [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            while not done:\n",
        "                obs_t = (torch.tensor(obs, dtype=torch.float32, device=cfg.device)\n",
        "                         .unsqueeze(0) / 255.0)                    # (1,64,64,3)\n",
        "                _, _, _, _, h, s = model.rssm.obs_step(h, s, obs_t, prev_a)\n",
        "                action = planner.plan(h, s, cfg.device).cpu().numpy()\n",
        "                prev_a = torch.tensor(action, dtype=torch.float32,\n",
        "                                      device=cfg.device).reshape(1, -1)\n",
        "                next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                obs_l.append(obs); act_l.append(action.reshape(-1))\n",
        "                rew_l.append(float(reward)); term_l.append(bool(done))\n",
        "                obs = next_obs\n",
        "        storage.add_episode(obs_l, act_l, rew_l, term_l)\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "TeSR21cYhhsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 17.  EVALUATE\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "def evaluate_planner(env, model: WorldModel, planner: CEMPlanner,\n",
        "                     n_episodes: int, config: Config) -> float:\n",
        "    \"\"\"Run n evaluation episodes; returns average reward.\"\"\"\n",
        "    model.eval()\n",
        "    cfg     = config\n",
        "    rewards = []\n",
        "    with torch.no_grad():\n",
        "        for ep in range(n_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            done   = False\n",
        "            h = torch.zeros(1, cfg.hidden_size, device=cfg.device)\n",
        "            s = torch.zeros(1, cfg.state_size,  device=cfg.device)\n",
        "            prev_a = torch.zeros(1, cfg.action_dim, device=cfg.device)\n",
        "            ep_r   = 0.0\n",
        "            while not done:\n",
        "                obs_t = (torch.tensor(obs, dtype=torch.float32, device=cfg.device)\n",
        "                         .unsqueeze(0) / 255.0)\n",
        "                _, _, _, _, h, s = model.rssm.obs_step(h, s, obs_t, prev_a)\n",
        "                action = planner.plan(h, s, cfg.device).cpu().numpy()\n",
        "                prev_a = torch.tensor(action, dtype=torch.float32,\n",
        "                                      device=cfg.device).reshape(1, -1)\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                done  = terminated or truncated\n",
        "                ep_r += reward\n",
        "            rewards.append(ep_r)\n",
        "            print(f\"  Eval Episode {ep+1}: {ep_r:.2f}\")\n",
        "    avg = float(np.mean(rewards))\n",
        "    print(f\"  Average: {avg:.2f}\")\n",
        "    model.train()\n",
        "    return avg"
      ],
      "metadata": {
        "id": "TRwBHK09howS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 15.  CHECKPOINTING\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "def save_checkpoint(model, optimizer, iteration: int, config: Config):\n",
        "    ckpt_dir = pathlib.Path(config.checkpoint_dir)\n",
        "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "    path = ckpt_dir / f'ckpt_{iteration:06d}.pt'\n",
        "    torch.save({'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'iteration': iteration}, path)\n",
        "    print(f\"  [Checkpoint] Saved → {path.name}\")\n",
        "    for old in sorted(ckpt_dir.glob('ckpt_*.pt'))[:-config.keep_checkpoints]:\n",
        "        old.unlink()\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, config: Config) -> int:\n",
        "    \"\"\"Returns iteration to resume from (0 = fresh start).\"\"\"\n",
        "    ckpt_dir  = pathlib.Path(config.checkpoint_dir)\n",
        "    all_ckpts = sorted(ckpt_dir.glob('ckpt_*.pt')) if ckpt_dir.exists() else []\n",
        "    if not all_ckpts:\n",
        "        return 0\n",
        "    data  = torch.load(all_ckpts[-1], map_location=config.device)\n",
        "    model.load_state_dict(data['model'])\n",
        "    optimizer.load_state_dict(data['optimizer'])\n",
        "    start = data['iteration'] + 1\n",
        "    print(f\"[Checkpoint] Resumed from {all_ckpts[-1].name}  (iter {start})\")\n",
        "    return start"
      ],
      "metadata": {
        "id": "Xw8Rey7jiKKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 18.  TRAINING LOOP\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "def train_planner(env, model, optimizer, planner, storage: EpisodeStorage, config):\n",
        "    device     = config.device\n",
        "    start_iter = load_checkpoint(model, optimizer, config)  # resume if checkpoint exists\n",
        "\n",
        "    # ── Phase 1: seed Drive with random episodes if the storage is empty ──\n",
        "    if len(storage) == 0:\n",
        "        print(\"Collecting initial random seed experience...\")\n",
        "        for _ in range(config.seed_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            done   = False\n",
        "            obs_l, act_l, rew_l, term_l = [], [], [], []\n",
        "            while not done:\n",
        "                action = env.action_space.sample()           # uniform random action\n",
        "                next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                obs_l.append(obs)\n",
        "                act_l.append(np.array(action, dtype=np.float32).reshape(-1))\n",
        "                rew_l.append(float(reward))\n",
        "                term_l.append(bool(done))\n",
        "                obs = next_obs\n",
        "            storage.add_episode(obs_l, act_l, rew_l, term_l)  # persist to Drive\n",
        "        print(f\"  Seeded {len(storage)} episodes.\")\n",
        "\n",
        "    # ── Phase 2: alternating train / collect ─────────────────────────────\n",
        "    #\n",
        "    # Persistent dataset — built ONCE here, then extended with add_episode().\n",
        "    # Previously the dataset was rebuilt inside the loop from all Drive files\n",
        "    # (O(n_episodes) reads per iteration).  Now only the single new episode\n",
        "    # file is read per iteration — O(1) Drive I/O regardless of dataset size.\n",
        "    virtual_len = config.batch_size * config.train_steps_per_iter\n",
        "    dataset     = EpisodeDataset(storage, seq_len=config.seq_len,\n",
        "                                 virtual_len=virtual_len)\n",
        "    loader      = DataLoader(\n",
        "        dataset,\n",
        "        batch_size  = config.batch_size,\n",
        "        shuffle     = False,         # randomness lives inside __getitem__\n",
        "        num_workers = 0,             # 0 avoids fork-deadlock in Colab\n",
        "        collate_fn  = collate_episode_batch,\n",
        "        pin_memory  = False,\n",
        "        drop_last   = True,\n",
        "    )\n",
        "\n",
        "    for iteration in range(start_iter, config.total_iterations):\n",
        "        print(f\"\\nIteration {iteration + 1}/{config.total_iterations}  \"\n",
        "              f\"| Episodes in storage: {len(storage)}\")\n",
        "\n",
        "        # ── Train ────────────────────────────────────────────────────────\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        steps_done = 0\n",
        "\n",
        "        for obs_b, act_b, rew_b, _ in loader:\n",
        "            if steps_done >= config.train_steps_per_iter:\n",
        "                break\n",
        "            obs_b = obs_b.to(device)\n",
        "            act_b = act_b.to(device)\n",
        "            rew_b = rew_b.to(device)\n",
        "            total_loss += train_step(model, optimizer, obs_b, act_b, rew_b, device)\n",
        "            steps_done += 1\n",
        "\n",
        "        print(f\"  Average Loss: {total_loss / max(steps_done, 1):.4f}  \"\n",
        "              f\"(over {steps_done} steps)\")\n",
        "\n",
        "        # ── Collect new episode and add it to the in-RAM dataset ──────────\n",
        "        # collect_experience writes to Drive (storage) and returns the path.\n",
        "        # We then call dataset.add_episode() so the new episode is immediately\n",
        "        # available for the next iteration without re-reading Drive.\n",
        "        collect_experience(env, model, planner, storage, config.collect_episodes, config)\n",
        "        # The newest path is always the last one appended to storage:\n",
        "        dataset.add_episode(storage.episode_paths[-1])\n",
        "\n",
        "        # ── Checkpoint ───────────────────────────────────────────────────\n",
        "        if (iteration + 1) % config.checkpoint_every == 0:\n",
        "            save_checkpoint(model, optimizer, iteration, config)\n",
        "\n"
      ],
      "metadata": {
        "id": "TNAv6hssiOeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 19.  DREAM VISUALIZER\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "def visualize_dream(model: WorldModel, storage: EpisodeStorage, config: Config,\n",
        "                    episode_idx=0, context_frames=5, dream_steps=50,\n",
        "                    save_gif=True, save_png=True):\n",
        "    \"\"\"\n",
        "    Open-loop prediction visualizer.\n",
        "    Context (green) — real obs fed through encoder → posterior warm-up.\n",
        "    Dream   (red)   — prior only, NO encoder, NO real pixels.\n",
        "    \"\"\"\n",
        "    import matplotlib; matplotlib.use('Agg')\n",
        "    import matplotlib.pyplot as plt\n",
        "    import imageio\n",
        "\n",
        "    viz_dir = pathlib.Path(config.viz_dir)\n",
        "    viz_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ep           = np.load(storage.episode_paths[episode_idx % len(storage)])\n",
        "    obs_ep       = ep['obs']\n",
        "    action_ep    = ep['action']\n",
        "    dream_steps  = max(0, min(dream_steps, len(obs_ep) - context_frames))\n",
        "    device       = config.device\n",
        "    cfg          = config\n",
        "    model.eval()\n",
        "\n",
        "    blank = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "    real_frames, recon_frames, dream_frames_list = [], [], []\n",
        "\n",
        "    h = torch.zeros(1, cfg.hidden_size, device=device)\n",
        "    s = torch.zeros(1, cfg.state_size,  device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for t in range(context_frames):\n",
        "            obs_t  = torch.tensor(obs_ep[t], dtype=torch.float32,\n",
        "                                  device=device).unsqueeze(0) / 255.0\n",
        "            prev_a = (torch.tensor(action_ep[t-1], dtype=torch.float32,\n",
        "                                   device=device).unsqueeze(0)\n",
        "                      if t > 0 else torch.zeros(1, cfg.action_dim, device=device))\n",
        "            _, _, _, _, h, s = model.rssm.obs_step(h, s, obs_t, prev_a)\n",
        "            recon = (model.decoder(h, s)[0].cpu().numpy()*255).clip(0,255).astype(np.uint8)\n",
        "            real_frames.append(obs_ep[t]); recon_frames.append(recon)\n",
        "\n",
        "        for d in range(dream_steps):\n",
        "            t   = context_frames + d\n",
        "            a_t = (torch.tensor(action_ep[t-1], dtype=torch.float32,\n",
        "                                device=device).unsqueeze(0)\n",
        "                   if t < len(action_ep) else torch.zeros(1, cfg.action_dim, device=device))\n",
        "            _, _, h, s = model.rssm.imagine_step(h, s, a_t)\n",
        "            dream = (model.decoder(h, s)[0].cpu().numpy()*255).clip(0,255).astype(np.uint8)\n",
        "            dream_frames_list.append(dream)\n",
        "            real_frames.append(obs_ep[t] if t < len(obs_ep) else blank)\n",
        "\n",
        "    recon_all = recon_frames + [blank] * dream_steps\n",
        "    gif_path = png_path = None\n",
        "\n",
        "    if save_gif:\n",
        "        SZ, SW, BH = 256, 10, 22\n",
        "        sep  = np.ones((SZ, SW, 3), np.uint8) * 200\n",
        "        labs = ['REAL (ground truth)', 'RECON (posterior)', 'DREAM (prior)']\n",
        "        frames = []\n",
        "        for i in range(len(real_frames)):\n",
        "            is_d = i >= context_frames\n",
        "            col  = (0,160,0) if not is_d else (180,0,0)\n",
        "            ps   = [cv2.resize(real_frames[i], (SZ,SZ), interpolation=cv2.INTER_NEAREST),\n",
        "                    cv2.resize(recon_all[i],   (SZ,SZ), interpolation=cv2.INTER_NEAREST),\n",
        "                    cv2.resize(dream_frames_list[i-context_frames] if is_d else blank,\n",
        "                               (SZ,SZ), interpolation=cv2.INTER_NEAREST)]\n",
        "            for pi, p in enumerate(ps):\n",
        "                p[:BH] = col\n",
        "                cv2.putText(p, labs[pi], (4,BH-5),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.42, (255,255,255), 1, cv2.LINE_AA)\n",
        "            cv2.putText(ps[0], f'{\"CONTEXT\" if not is_d else \"DREAM\"}  t={i}',\n",
        "                        (4,BH+18), cv2.FONT_HERSHEY_SIMPLEX, 0.46, (255,255,0), 1, cv2.LINE_AA)\n",
        "            frames.append(np.hstack([ps[0], sep, ps[1], sep, ps[2]]))\n",
        "        gif_path = viz_dir / f'dream_ep{episode_idx:04d}.gif'\n",
        "        imageio.mimsave(str(gif_path), frames, fps=5, loop=0)\n",
        "        print(f\"[Viz] GIF → {gif_path}\")\n",
        "\n",
        "    if save_png:\n",
        "        N=5; ctx=list(range(context_frames))\n",
        "        early=list(range(context_frames, context_frames+min(N,dream_steps)))\n",
        "        ls=context_frames+dream_steps-min(N,dream_steps)\n",
        "        late=list(range(ls, context_frames+dream_steps))\n",
        "        gap=(context_frames+N)<ls\n",
        "        cols=ctx+early+(['gap'] if gap else [])+late; nc=len(cols)\n",
        "        fig=plt.figure(figsize=(1.6+nc*3, 3*3+1.8), facecolor='#181818')\n",
        "        gs=fig.add_gridspec(3,nc, left=1.6/(1.6+nc*3), right=0.99,\n",
        "                            top=0.87, bottom=0.06, wspace=0.07, hspace=0.22)\n",
        "        RM=[('Real\\nGround Truth','#43a047'),\n",
        "            ('Recon\\nPosterior q(s|h,e)','#1e88e5'),\n",
        "            ('Dream\\nPrior p(s|h) only','#e53935')]\n",
        "        for ri,(rl,rc) in enumerate(RM):\n",
        "            for ci,ti in enumerate(cols):\n",
        "                ax=fig.add_subplot(gs[ri,ci])\n",
        "                ax.set_facecolor('#181818'); ax.set_xticks([]); ax.set_yticks([])\n",
        "                if ti=='gap':\n",
        "                    ax.axis('off')\n",
        "                    ax.text(0.5,0.5,'···\\nskipped',ha='center',va='center',\n",
        "                            fontsize=13,color='#888',transform=ax.transAxes,fontstyle='italic')\n",
        "                    if ri==0: ax.set_title('···',fontsize=12,color='#666',pad=7)\n",
        "                    continue\n",
        "                ic=ti<context_frames; bc='#66bb6a' if ic else '#ef5350'\n",
        "                if   ri==0: img=real_frames[ti]\n",
        "                elif ri==1: img=recon_all[ti] if ic else blank\n",
        "                else:       img=dream_frames_list[ti-context_frames] if not ic else blank\n",
        "                ax.imshow(img, interpolation='nearest', aspect='equal')\n",
        "                for sp in ax.spines.values():\n",
        "                    sp.set_visible(True); sp.set_edgecolor(bc); sp.set_linewidth(3.5)\n",
        "                if ri==0:\n",
        "                    ax.set_title(f't={ti}\\n[{\"CTX\" if ic else \"DREAM\"}]',\n",
        "                                 fontsize=12,pad=6,color=bc,fontweight='bold')\n",
        "            yf=1.0-(ri+0.5)/3.0\n",
        "            fig.text(0.005, 0.06+yf*0.81, rl, ha='left', va='center',\n",
        "                     fontsize=12, color=rc, fontweight='bold',\n",
        "                     rotation=90, rotation_mode='anchor')\n",
        "        fig.suptitle(f'Dream  ·  {config.env_name}  ·  Ep {episode_idx}  ·  '\n",
        "                     f'{context_frames} context + {dream_steps} dream',\n",
        "                     fontsize=14, color='white', y=0.97)\n",
        "        from matplotlib.patches import Patch\n",
        "        fig.legend(handles=[Patch(facecolor='#66bb6a',label='Context'),\n",
        "                             Patch(facecolor='#ef5350',label='Dream')],\n",
        "                   loc='lower center', ncol=2, fontsize=11,\n",
        "                   facecolor='#2a2a2a', edgecolor='#555', labelcolor='white',\n",
        "                   bbox_to_anchor=(0.5,0.0))\n",
        "        png_path=viz_dir/f'dream_grid_ep{episode_idx:04d}.png'\n",
        "        fig.savefig(str(png_path), dpi=180, bbox_inches='tight',\n",
        "                    facecolor=fig.get_facecolor())\n",
        "        plt.close(fig)\n",
        "        print(f\"[Viz] PNG → {png_path}\")\n",
        "    return gif_path, png_path\n"
      ],
      "metadata": {
        "id": "z5VvZcBTiU2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "# 20.  ENTRY POINT\n",
        "# ══════════════════════════════════════════════════════════════════════════════\n",
        "if __name__ == '__main__':\n",
        "    # ── Colab: start a virtual display for MuJoCo rendering ──────────────────\n",
        "    try:\n",
        "        from pyvirtualdisplay import Display\n",
        "        Display(visible=0, size=(1400, 900)).start()\n",
        "        print('[Display] Virtual display started')\n",
        "    except ImportError:\n",
        "        print('[Display] pyvirtualdisplay not found — assuming local display')\n",
        "\n",
        "    # ── Mount Google Drive (Colab only) ───────────────────────────────────────\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print('[Drive] Mounted')\n",
        "    except ImportError:\n",
        "        Config.drive_base     = './local_storage'\n",
        "        Config.episode_dir    = f'{Config.drive_base}/{Config.env_name}/episodes'\n",
        "        Config.checkpoint_dir = f'{Config.drive_base}/{Config.env_name}/checkpoints'\n",
        "        Config.viz_dir        = f'{Config.drive_base}/{Config.env_name}/visualizations'\n",
        "\n",
        "    config = Config()\n",
        "    device = config.device\n",
        "    print(f'[Config] Device: {device}  |  Env: {config.env_name}'\n",
        "          f'  |  action_dim={config.action_dim}'\n",
        "          f'  |  hidden={config.hidden_size}  state={config.state_size}')\n",
        "\n",
        "    # ── Environment ───────────────────────────────────────────────────────────\n",
        "    base_env = gym.make(config.env_name, render_mode='rgb_array')\n",
        "    env      = PixelWrapper(base_env, render_size=64)\n",
        "\n",
        "    # ── Model — all dims driven by cfg, no magic numbers ─────────────────────\n",
        "    model     = WorldModel(config).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config.lr)\n",
        "    planner   = CEMPlanner(model, config)\n",
        "    storage   = EpisodeStorage(config.episode_dir)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'[Model] {total_params:,} parameters')\n",
        "\n",
        "    # ── Train ─────────────────────────────────────────────────────────────────\n",
        "    train_planner(env, model, optimizer, planner, storage, config)\n",
        "\n",
        "    # ── Evaluate & checkpoint ─────────────────────────────────────────────────\n",
        "    evaluate_planner(env, model, planner, 5, config)\n",
        "    save_checkpoint(model, optimizer, config.total_iterations - 1, config)\n",
        "\n",
        "    # ── Visualize ─────────────────────────────────────────────────────────────\n",
        "    for ep_idx in range(min(3, len(storage))):\n",
        "        visualize_dream(model, storage, config,\n",
        "                        episode_idx=ep_idx, context_frames=5, dream_steps=50)\n",
        "    print('Done. Check Drive at:', config.viz_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqZhmWl1iZ7o",
        "outputId": "5f5cc09f-79cc-4b47-e773-d3c578e9cf68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Display] Virtual display started\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Drive] Mounted\n",
            "[Config] Device: cuda  |  Env: HalfCheetah-v4  |  action_dim=6  |  hidden=400  state=50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py:512: DeprecationWarning: \u001b[33mWARN: The environment HalfCheetah-v4 is out of date. You should consider upgrading to version `v5`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Storage] 147 existing episodes in /content/drive/MyDrive/PlaNet/HalfCheetah-v4/episodes\n",
            "[Model] 8,832,316 parameters\n",
            "[Checkpoint] Resumed from ckpt_000119.pt  (iter 120)\n",
            "[EpisodeDataset] Cached 147 episodes (1806.3 MB) | virtual_len=5000\n",
            "  Eval Episode 1: -65.91\n",
            "  Eval Episode 2: -23.78\n",
            "  Eval Episode 3: -54.14\n",
            "  Eval Episode 4: -31.31\n",
            "  Eval Episode 5: -53.19\n",
            "  Average: -45.67\n",
            "  [Checkpoint] Saved → ckpt_000119.pt\n",
            "[Viz] GIF → /content/drive/MyDrive/PlaNet/HalfCheetah-v4/visualizations/dream_ep0000.gif\n",
            "[Viz] PNG → /content/drive/MyDrive/PlaNet/HalfCheetah-v4/visualizations/dream_grid_ep0000.png\n",
            "[Viz] GIF → /content/drive/MyDrive/PlaNet/HalfCheetah-v4/visualizations/dream_ep0001.gif\n",
            "[Viz] PNG → /content/drive/MyDrive/PlaNet/HalfCheetah-v4/visualizations/dream_grid_ep0001.png\n",
            "[Viz] GIF → /content/drive/MyDrive/PlaNet/HalfCheetah-v4/visualizations/dream_ep0002.gif\n",
            "[Viz] PNG → /content/drive/MyDrive/PlaNet/HalfCheetah-v4/visualizations/dream_grid_ep0002.png\n",
            "Done. Check Drive at: /content/drive/MyDrive/PlaNet/HalfCheetah-v4/visualizations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "#  VISUALIZE DREAM TO END OF EPISODE\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def visualize_dream_to_end(model, storage, config, episode_idx=0, context_frames=5):\n",
        "    \"\"\"\n",
        "    Open-loop prediction visualizer that runs until the end of the episode.\n",
        "    Context (green) — real obs fed through encoder → posterior warm-up.\n",
        "    Dream   (red)   — prior only, NO encoder, NO real pixels.\n",
        "    Runs for all remaining steps in the TRUE recorded episode.\n",
        "    \"\"\"\n",
        "    viz_dir = pathlib.Path(config.viz_dir)\n",
        "    viz_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    episode_idx = episode_idx % len(storage)\n",
        "    ep          = np.load(storage.episode_paths[episode_idx])\n",
        "    obs_ep      = ep['obs']\n",
        "    action_ep   = ep['action']\n",
        "\n",
        "    # We dream for exactly how many steps are left in the recorded episode\n",
        "    dream_steps = max(0, len(obs_ep) - context_frames)\n",
        "\n",
        "    device      = config.device\n",
        "    model.eval()\n",
        "\n",
        "    real_frames, recon_frames, dream_frames_list = [], [], []\n",
        "    blank = np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "\n",
        "    # Note: MuJoCo models use cfg.hidden_size, pendulums use fixed 200\n",
        "    h_size = getattr(config, 'hidden_size', 200)\n",
        "    s_size = getattr(config, 'state_size', 30)\n",
        "    a_dim  = getattr(config, 'action_dim', 1)\n",
        "\n",
        "    h = torch.zeros(1, h_size, device=device)\n",
        "    s = torch.zeros(1, s_size, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for t in range(context_frames):\n",
        "            obs_t  = torch.tensor(obs_ep[t], dtype=torch.float32, device=device).unsqueeze(0) / 255.0\n",
        "            prev_a = torch.tensor(action_ep[t-1], dtype=torch.float32, device=device).unsqueeze(0) if t > 0 else torch.zeros(1, a_dim, device=device)\n",
        "            _, _, _, _, h, s = model.rssm.obs_step(h, s, obs_t, prev_a)\n",
        "            recon = (model.decoder(h, s)[0].cpu().numpy()*255).clip(0,255).astype(np.uint8)\n",
        "            real_frames.append(obs_ep[t])\n",
        "            recon_frames.append(recon)\n",
        "\n",
        "        for d in range(dream_steps):\n",
        "            t   = context_frames + d\n",
        "            # We use the TRUE action recorded from the episode to feed into the dream\n",
        "            a_t = torch.tensor(action_ep[t-1], dtype=torch.float32, device=device).unsqueeze(0) if t < len(action_ep) else torch.zeros(1, a_dim, device=device)\n",
        "            _, _, h, s = model.rssm.imagine_step(h, s, a_t)\n",
        "            dream = (model.decoder(h, s)[0].cpu().numpy()*255).clip(0,255).astype(np.uint8)\n",
        "            dream_frames_list.append(dream)\n",
        "            real_frames.append(obs_ep[t] if t < len(obs_ep) else blank)\n",
        "\n",
        "    recon_all = recon_frames + [blank] * dream_steps\n",
        "\n",
        "    SZ, SW, BH = 256, 10, 22\n",
        "    sep  = np.ones((SZ, SW, 3), np.uint8) * 200\n",
        "    cols = ['REAL (ground truth)', 'RECON (posterior)', 'DREAM (prior)']\n",
        "    frames = []\n",
        "\n",
        "    for i in range(len(real_frames)):\n",
        "        is_d   = i >= context_frames\n",
        "        colour = (0,160,0) if not is_d else (180,0,0)\n",
        "        panels = [\n",
        "            cv2.resize(real_frames[i],                                      (SZ,SZ), interpolation=cv2.INTER_NEAREST),\n",
        "            cv2.resize(recon_all[i],                                        (SZ,SZ), interpolation=cv2.INTER_NEAREST),\n",
        "            cv2.resize(dream_frames_list[i-context_frames] if is_d else blank, (SZ,SZ), interpolation=cv2.INTER_NEAREST),\n",
        "        ]\n",
        "        for pi, p in enumerate(panels):\n",
        "            p[:BH] = colour\n",
        "            cv2.putText(p, cols[pi], (4, BH-5), cv2.FONT_HERSHEY_SIMPLEX, 0.42, (255,255,255), 1, cv2.LINE_AA)\n",
        "        cv2.putText(panels[0], f'{\"CONTEXT\" if not is_d else \"DREAM\"}  t={i}',\n",
        "                    (4, BH+18), cv2.FONT_HERSHEY_SIMPLEX, 0.46, (255,255,0), 1, cv2.LINE_AA)\n",
        "        frames.append(np.hstack([panels[0], sep, panels[1], sep, panels[2]]))\n",
        "\n",
        "    gif_path = viz_dir / f'dream_full_ep{episode_idx:04d}.gif'\n",
        "    imageio.mimsave(str(gif_path), frames, fps=15, loop=0)\n",
        "    print(f\"[Viz] Full Dream GIF → {gif_path}\")\n",
        "    return gif_path\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  VISUALIZE REAL AGENT ROLLOUT USING TRAINED PLANNER\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def visualize_real_agent_rollout(env, model, planner, config, max_steps=1000):\n",
        "    \"\"\"\n",
        "    Visualizes the real agent interacting with the environment using the trained model's planner.\n",
        "    Note: This runs a LIVE simulation in the environment, rather than reading from storage.\n",
        "    \"\"\"\n",
        "    viz_dir = pathlib.Path(config.viz_dir)\n",
        "    viz_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    device = config.device\n",
        "    model.eval()\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    h_size = getattr(config, 'hidden_size', 200)\n",
        "    s_size = getattr(config, 'state_size', 30)\n",
        "    a_dim  = getattr(config, 'action_dim', 1)\n",
        "\n",
        "    h = torch.zeros(1, h_size, device=device)\n",
        "    s = torch.zeros(1, s_size,  device=device)\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    # 1. Warm up the first step\n",
        "    with torch.no_grad():\n",
        "        init_obs = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0) / 255.0\n",
        "        dummy_a  = torch.zeros(1, a_dim, device=device)\n",
        "        _, _, _, _, h, s = model.rssm.obs_step(h, s, init_obs[0], dummy_a)\n",
        "        frames.append(obs)\n",
        "\n",
        "    steps = 0\n",
        "    while not done and steps < max_steps:\n",
        "        with torch.no_grad():\n",
        "            # Support both architectures\n",
        "            action = planner.plan(h, s) if not hasattr(planner, 'cfg') else planner.plan(h, s, device)\n",
        "            action_np = action.cpu().numpy().reshape(-1)\n",
        "\n",
        "            # Step the real environment\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action_np)\n",
        "            done = terminated or truncated\n",
        "            frames.append(next_obs)\n",
        "\n",
        "            # Feed the real frame back into the model to prepare for the *next* plan\n",
        "            obs_t = torch.tensor(next_obs, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0) / 255.0\n",
        "            act_t = action.unsqueeze(0).unsqueeze(0)\n",
        "            _, _, _, _, h, s = model.rssm.obs_step(h, s, obs_t[0], act_t[0])\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "    SZ, SW, BH = 256, 10, 22\n",
        "    gif_frames = []\n",
        "\n",
        "    for i, frame in enumerate(frames):\n",
        "        panel = cv2.resize(frame, (SZ, SZ), interpolation=cv2.INTER_NEAREST)\n",
        "        panel[:BH] = (0, 160, 0)\n",
        "\n",
        "        cv2.putText(panel, 'AGENT LIVE ROLLOUT', (4, BH-5), cv2.FONT_HERSHEY_SIMPLEX, 0.42, (255,255,255), 1, cv2.LINE_AA)\n",
        "        cv2.putText(panel, f'Step: {i}', (4, BH+18), cv2.FONT_HERSHEY_SIMPLEX, 0.46, (255,255,0), 1, cv2.LINE_AA)\n",
        "\n",
        "        if done and i == len(frames) - 1:\n",
        "            panel[:BH] = (0, 0, 160)\n",
        "            cv2.putText(panel, 'TERMINATED', (4, BH-5), cv2.FONT_HERSHEY_SIMPLEX, 0.42, (255,255,255), 1, cv2.LINE_AA)\n",
        "\n",
        "        gif_frames.append(panel)\n",
        "\n",
        "    gif_path = viz_dir / f'agent_live_rollout.gif'\n",
        "\n",
        "    # We use a higher FPS (usually 30 or 50) for a smoother playback since it's a real rollout\n",
        "    imageio.mimsave(str(gif_path), gif_frames, fps=30, loop=0)\n",
        "    print(f\"[Viz] Agent Rollout GIF ({len(frames)} frames) → {gif_path}\")\n",
        "\n",
        "    return gif_path\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "#  RUN VISUALIZATIONS WITHOUT TRAINING\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        from pyvirtualdisplay import Display\n",
        "        Display(visible=0, size=(1400, 900)).start()\n",
        "        print('[Display] Virtual display started')\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    config = Config()\n",
        "    device = config.device\n",
        "    print(f\"\\n[Viz] Starting standalone visualization on {device}...\")\n",
        "\n",
        "    # 1. Environment (matches planet.py)\n",
        "    base_env = gym.make(config.env_name, render_mode='rgb_array')\n",
        "    env      = PixelWrapper(base_env, render_size=64)\n",
        "\n",
        "    # 2. Model & Storage\n",
        "    # ── Model — all dims driven by cfg, no magic numbers ─────────────────────\n",
        "    model     = WorldModel(config).to(device)\n",
        "    planner   = CEMPlanner(model, config)\n",
        "    storage   = EpisodeStorage(config.episode_dir)\n",
        "\n",
        "    # 3. Load Checkpoint\n",
        "    dummy_optimizer = optim.AdamW(model.parameters(), lr=1e-4) # required for signature\n",
        "    start_iter = load_checkpoint(model, dummy_optimizer, config)\n",
        "    print(f\"[Viz] Loaded model weights from iteration: {start_iter}\")\n",
        "\n",
        "    print(\"\\n--- Running Open-Loop Dream to End ---\")\n",
        "    if len(storage) > 0:\n",
        "        visualize_dream_to_end(model, storage, config, episode_idx=len(storage)-1, context_frames=5)\n",
        "    else:\n",
        "        print(\"[Viz] Empty storage - skipping dream visualization.\")\n",
        "\n",
        "    print(\"\\n--- Running Live Agent Rollout ---\")\n",
        "    visualize_real_agent_rollout(env, model, planner, config, max_steps=150)\n",
        "\n",
        "    print(\"\\n[Viz] All Done! Check your Drive.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bahGACIsMDUL",
        "outputId": "ae0b631d-34a0-458f-fb71-600f4b992cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Display] Virtual display started\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "[Viz] Starting standalone visualization on cuda...\n",
            "[Storage] 147 existing episodes in /content/drive/MyDrive/PlaNet/HalfCheetah-v4/episodes\n",
            "[Checkpoint] Resumed from ckpt_000119.pt  (iter 120)\n",
            "[Viz] Loaded model weights from iteration: 120\n",
            "\n",
            "--- Running Open-Loop Dream to End ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/envs/registration.py:512: DeprecationWarning: \u001b[33mWARN: The environment HalfCheetah-v4 is out of date. You should consider upgrading to version `v5`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Viz] Full Dream GIF → /content/drive/MyDrive/PlaNet/HalfCheetah-v4/visualizations/dream_full_ep0146.gif\n",
            "\n",
            "--- Running Live Agent Rollout ---\n",
            "[Viz] Agent Rollout GIF (151 frames) → /content/drive/MyDrive/PlaNet/HalfCheetah-v4/visualizations/agent_live_rollout.gif\n",
            "\n",
            "[Viz] All Done! Check your Drive.\n"
          ]
        }
      ]
    }
  ]
}