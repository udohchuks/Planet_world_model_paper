{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PlaNet — Learning Latent Dynamics for Planning from Pixels\n",
                "**Paper:** Hafner et al. (2019) · [arxiv.org/abs/1811.04551](https://arxiv.org/abs/1811.04551)\n",
                "\n",
                "PlaNet learns a **world model** that predicts the future in a compressed latent space, then plans inside that space using the Cross-Entropy Method (CEM) — never planning in pixel space.\n",
                "\n",
                "```\n",
                "Pixel obs ──► Encoder ──► Posterior q(s|h,e) ──► (h, s)\n",
                "                                                     │\n",
                "                            GRU  h_t = f(s_{t-1}, a_{t-1}, h_{t-1})\n",
                "                                                     │\n",
                "                              Prior p(s|h) ◄── planning / dreaming\n",
                "                                                     │\n",
                "                            Decoder + Reward ──► reconstruct obs & reward\n",
                "```\n",
                "\n",
                "**File layout**\n",
                "```\n",
                "tutorial/\n",
                "├── utils.py        ← PixelWrapper, EpisodeStorage, checkpoints,\n",
                "│                       seed_storage, collect_episode, run_eval, visualize_dream\n",
                "└── tutorial.ipynb  ← (this file) Config, all model architecture, training loop\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 0: Install ────────────────────────────────────────────────────────────\n",
                "!pip install -q gymnasium imageio imageio-ffmpeg matplotlib opencv-python-headless"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 1: Mount Google Drive ────────────────────────────────────────────────\n",
                "# All episodes and checkpoints are saved to Drive so training survives\n",
                "# Colab runtime restarts.  On a local machine this block is skipped.\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive', force_remount=False)\n",
                "    IN_COLAB = True\n",
                "except ImportError:\n",
                "    IN_COLAB = False\n",
                "    print('Not in Colab — will save locally.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 2: Imports ───────────────────────────────────────────────────────────\n",
                "import sys\n",
                "sys.path.insert(0, '/content/tutorial' if IN_COLAB else '.')\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.nn.utils import clip_grad_norm_\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import gymnasium as gym\n",
                "\n",
                "# Plumbing (file I/O, env wrapper, data collection, visualizer)\n",
                "from utils import (PixelWrapper, EpisodeStorage,\n",
                "                   save_checkpoint, load_checkpoint,\n",
                "                   seed_storage, collect_episode, run_eval,\n",
                "                   visualize_dream)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 3: Config ────────────────────────────────────────────────────────────\n",
                "class Config:\n",
                "    # Hardware\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "    # Environment — Pendulum-v1 has a single continuous torque action\n",
                "    action_dim = 1\n",
                "    obs_size   = 64   # pixel H = W\n",
                "\n",
                "    # Training schedule\n",
                "    total_iterations     = 100  # outer loop: train → collect → repeat\n",
                "    train_steps_per_iter = 50   # gradient steps per iteration\n",
                "    batch_size           = 16\n",
                "    seq_len              = 25   # timesteps per training sequence\n",
                "    lr                   = 1e-4\n",
                "\n",
                "    # Data collection\n",
                "    seed_episodes    = 5   # random episodes before CEM is used\n",
                "    collect_episodes = 1   # CEM episodes collected per iteration\n",
                "\n",
                "    # Persistence (Google Drive)\n",
                "    drive_base     = '/content/drive/MyDrive/PlaNet'\n",
                "    episode_dir    = f'{drive_base}/episodes'\n",
                "    checkpoint_dir = f'{drive_base}/checkpoints'\n",
                "    viz_dir        = f'{drive_base}/visualizations'\n",
                "    checkpoint_every  = 10\n",
                "    keep_checkpoints  = 5\n",
                "\n",
                "config = Config()\n",
                "print(f'Device: {config.device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Model Architecture\n",
                "\n",
                "| Module | Input → Output | Role |\n",
                "|--------|---------------|------|\n",
                "| **Encoder** | `(B,64,64,3)` → `(B,1024)` | Compress pixels to embedding |\n",
                "| **GRU** | `(B,30+1)` → `(B,200)` | Track deterministic history `h_t` |\n",
                "| **Prior** | `(B,200)` → `(B,30)×2` | Predict `s_t` from `h_t` only |\n",
                "| **Posterior** | `(B,1224)` → `(B,30)×2` | Refine `s_t` using observation |\n",
                "| **Decoder** | `(B,230)` → `(B,64,64,3)` | Reconstruct pixels from `(h,s)` |\n",
                "| **Reward** | `(B,230)` → `(B,1)` | Predict reward from `(h,s)` |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 4: Encoder ──────────────────────────────────────────────────────────\n",
                "# Four strided convolutions progressively halve spatial resolution:\n",
                "#   64×64 → 32×32 → 16×16 → 8×8 → 4×4  (each with kernel=4, stride=2, pad=1)\n",
                "# A final linear layer flattens 256×4×4=4096 features → 1024-D embedding.\n",
                "# This embedding is passed to the Posterior to infer the stochastic state s_t.\n",
                "class Encoder(nn.Module):\n",
                "    def __init__(self, in_channels=3):\n",
                "        super().__init__()\n",
                "        self.cv1 = nn.Conv2d(in_channels, 32,  4, 2, 1)  # → (B,32,32,32)\n",
                "        self.cv2 = nn.Conv2d(32,          64,  4, 2, 1)  # → (B,64,16,16)\n",
                "        self.cv3 = nn.Conv2d(64,          128, 4, 2, 1)  # → (B,128,8,8)\n",
                "        self.cv4 = nn.Conv2d(128,         256, 4, 2, 1)  # → (B,256,4,4)\n",
                "        self.fc  = nn.Linear(256 * 4 * 4, 1024)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: (B, H, W, C)  —  Conv2d expects (B, C, H, W)\n",
                "        x = x.permute(0, 3, 1, 2)\n",
                "        x = F.relu(self.cv1(x))\n",
                "        x = F.relu(self.cv2(x))\n",
                "        x = F.relu(self.cv3(x))\n",
                "        x = F.relu(self.cv4(x))\n",
                "        return self.fc(x.flatten(1))   # → (B, 1024)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 5: GRU · Prior · Posterior · RSSM ───────────────────────────────────\n",
                "#\n",
                "# The RSSM maintains two parallel state components:\n",
                "#\n",
                "#   Deterministic  h_t  — computed by the GRU from (s_{t-1}, a_{t-1}, h_{t-1})\n",
                "#                         Carries long-range temporal information.\n",
                "#\n",
                "#   Stochastic     s_t  — sampled from either:\n",
                "#       Posterior q(s|h,e): uses encoded obs during *training* (more accurate)\n",
                "#       Prior     p(s|h):   no obs required, used for *planning / dreaming*\n",
                "#\n",
                "# The KL loss KL[q || p] forces the prior to track the posterior, making the\n",
                "# prior reliable enough to plan with.\n",
                "\n",
                "class GRU(nn.Module):\n",
                "    \"\"\"h_t = GRUCell([s_{t-1}, a_{t-1}], h_{t-1})\"\"\"\n",
                "    def __init__(self, state_size=30, action_dim=1, hidden_size=200):\n",
                "        super().__init__()\n",
                "        # Input is the concatenation of stochastic state and action\n",
                "        # (B, 30+1=31) → (B, 200)\n",
                "        self.cell = nn.GRUCell(state_size + action_dim, hidden_size)\n",
                "\n",
                "    def forward(self, s, a, h):\n",
                "        return self.cell(torch.cat([s, a], dim=-1), h)  # (B,200)\n",
                "\n",
                "\n",
                "class Prior(nn.Module):\n",
                "    \"\"\"p(s_t | h_t) — prediction from deterministic state alone.\n",
                "    Returns (mean, std) of a diagonal Gaussian over s_t.\n",
                "    The +0.1 floor on std prevents collapse (zero variance → infinite KL).\n",
                "    \"\"\"\n",
                "    def __init__(self, hidden=200, out=30):\n",
                "        super().__init__()\n",
                "        self.fc  = nn.Linear(hidden, 256)  # (B,200) → (B,256)\n",
                "        self.mu  = nn.Linear(256, out)     # → (B,30)\n",
                "        self.std = nn.Linear(256, out)     # → (B,30)  (before softplus)\n",
                "\n",
                "    def forward(self, h):\n",
                "        x = F.relu(self.fc(h))\n",
                "        return self.mu(x), F.softplus(self.std(x)) + 0.1\n",
                "\n",
                "\n",
                "class Posterior(nn.Module):\n",
                "    \"\"\"q(s_t | h_t, e_t) — uses the encoded obs to improve the state estimate.\n",
                "    Input is the concatenation of h (200) and e (1024) → 1224 features.\n",
                "    \"\"\"\n",
                "    def __init__(self, out=30):\n",
                "        super().__init__()\n",
                "        self.fc  = nn.Linear(1024 + 200, 256)  # (B,1224) → (B,256)\n",
                "        self.mu  = nn.Linear(256, out)\n",
                "        self.std = nn.Linear(256, out)\n",
                "\n",
                "    def forward(self, e, h):\n",
                "        x = F.relu(self.fc(torch.cat([e, h], dim=-1)))\n",
                "        return self.mu(x), F.softplus(self.std(x)) + 0.1\n",
                "\n",
                "\n",
                "class RSSM(nn.Module):\n",
                "    \"\"\"Recurrent State Space Model — the core of PlaNet.\"\"\"\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.encoder   = Encoder()\n",
                "        self.gru       = GRU()\n",
                "        self.prior     = Prior()\n",
                "        self.posterior = Posterior()\n",
                "\n",
                "    def obs_step(self, h, s, obs, a_prev):\n",
                "        \"\"\"One training step — uses real observation via posterior.\n",
                "        Reparameterisation trick: s = mu + std * eps, eps ~ N(0,I)\n",
                "        allows gradients to flow through the sampling operation.\n",
                "        \"\"\"\n",
                "        h          = self.gru(s, a_prev, h)            # (B,200)\n",
                "        e          = self.encoder(obs)                  # (B,1024)\n",
                "        p_m, p_s   = self.prior(h)                     # (B,30) each\n",
                "        q_m, q_s   = self.posterior(e, h)              # (B,30) each\n",
                "        s          = q_m + q_s * torch.randn_like(q_m) # (B,30)  reparameterised\n",
                "        return p_m, p_s, q_m, q_s, h, s\n",
                "\n",
                "    def imagine_step(self, h, s, a):\n",
                "        \"\"\"One dream/planning step — prior only, NO encoder called.\"\"\"\n",
                "        h        = self.gru(s, a, h)\n",
                "        p_m, p_s = self.prior(h)\n",
                "        s        = p_m + p_s * torch.randn_like(p_m)\n",
                "        return p_m, p_s, h, s"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 6: Decoder · Reward ─────────────────────────────────────────────────\n",
                "#\n",
                "# Decoder mirrors the Encoder architecture using transposed convolutions.\n",
                "# Input: concat(h, s) → (B, 230) → linear → (B, 4096) → reshape → (B,256,4,4)\n",
                "# Then progressively upscale: 4×4 → 8×8 → 16×16 → 32×32 → 64×64\n",
                "# Sigmoid output keeps pixel values in [0, 1] matching the normalised input.\n",
                "#\n",
                "# Reward head is a small 3-layer MLP over (h, s) → scalar.\n",
                "# Training the reward head ensures the latent state encodes task-relevant info.\n",
                "\n",
                "class Decoder(nn.Module):\n",
                "    def __init__(self, state=30, hidden=200):\n",
                "        super().__init__()\n",
                "        self.fc = nn.Linear(state + hidden, 4096)        # (B,230) → (B,4096)\n",
                "        self.d1 = nn.ConvTranspose2d(256, 128, 4, 2, 1)  # → (B,128,8,8)\n",
                "        self.d2 = nn.ConvTranspose2d(128, 64,  4, 2, 1)  # → (B,64,16,16)\n",
                "        self.d3 = nn.ConvTranspose2d(64,  32,  4, 2, 1)  # → (B,32,32,32)\n",
                "        self.d4 = nn.ConvTranspose2d(32,  3,   4, 2, 1)  # → (B,3,64,64)\n",
                "\n",
                "    def forward(self, h, s):\n",
                "        x = F.relu(self.fc(torch.cat([h, s], dim=-1))).reshape(-1, 256, 4, 4)\n",
                "        x = F.relu(self.d1(x))\n",
                "        x = F.relu(self.d2(x))\n",
                "        x = F.relu(self.d3(x))\n",
                "        # permute back to (B, H, W, C) to match obs format throughout the code\n",
                "        return torch.sigmoid(self.d4(x)).permute(0, 2, 3, 1)  # (B,64,64,3)\n",
                "\n",
                "\n",
                "class Reward(nn.Module):\n",
                "    def __init__(self, state=30, hidden=200, dim=400):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(state + hidden, dim), nn.ReLU(),  # (B,230) → (B,400)\n",
                "            nn.Linear(dim, dim),            nn.ReLU(),  # (B,400) → (B,400)\n",
                "            nn.Linear(dim, 1),                          # (B,400) → (B,1)\n",
                "        )\n",
                "\n",
                "    def forward(self, h, s):\n",
                "        return self.net(torch.cat([s, h], dim=-1))  # (B,1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 7: World Model ───────────────────────────────────────────────────────\n",
                "#\n",
                "# Wraps RSSM + Decoder + Reward and processes full (T, B) sequences.\n",
                "# Returns everything needed to compute the loss:\n",
                "#   recons       : (T, B, 64, 64, 3) — pixel reconstructions\n",
                "#   pred_rewards : (T, B, 1)         — predicted rewards\n",
                "#   prior params : (T, B, 30) each   — p_mean, p_std\n",
                "#   post  params : (T, B, 30) each   — q_mean, q_std\n",
                "#   overshoot_kl : scalar            — latent overshooting regulariser\n",
                "#\n",
                "# Latent Overshooting (Section 3.3 of the paper)\n",
                "# ─────────────────────────────────────────────────\n",
                "# A standard 1-step KL [ q(s_t | h_t, e_t) || p(s_t | h_t) ] only teaches\n",
                "# the prior to be accurate 1 step ahead — but CEM plans 12 steps out.\n",
                "#\n",
                "# Overshooting forces the prior to also match the posterior at depths 1…D:\n",
                "#   KL[ p(s_{t+d} | imagine_d(h_t)) || q(s_{t+d} | h_{t+d}, e_{t+d}) ]\n",
                "# This regularises the prior across multiple steps, so the model's\n",
                "# imagination stays reliable long into the future.\n",
                "\n",
                "class WorldModel(nn.Module):\n",
                "    def __init__(self, overshoot_d=5):\n",
                "        super().__init__()\n",
                "        self.rssm        = RSSM()\n",
                "        self.decoder     = Decoder()\n",
                "        self.reward_head = Reward()\n",
                "        self.overshoot_d = overshoot_d\n",
                "\n",
                "    def forward(self, obs_seq, action_seq):\n",
                "        # obs_seq    : (T, B, 64, 64, 3) float32 in [0, 1]\n",
                "        # action_seq : (T, B, action_dim)\n",
                "        T, B   = obs_seq.shape[:2]\n",
                "        device = obs_seq.device\n",
                "\n",
                "        # Initialise hidden states to zero — no prior episode history\n",
                "        h = torch.zeros(B, 200, device=device)  # deterministic state\n",
                "        s = torch.zeros(B,  30, device=device)  # stochastic  state\n",
                "\n",
                "        recons, rewards         = [], []\n",
                "        p_means, p_stds         = [], []\n",
                "        q_means, q_stds         = [], []\n",
                "        h_all,   s_all          = [], []\n",
                "\n",
                "        for t in range(T):\n",
                "            # Use previous action as context (a_{t-1}); zeros at t=0\n",
                "            a_prev = action_seq[t-1] if t > 0 else torch.zeros_like(action_seq[0])\n",
                "            p_m, p_s, q_m, q_s, h, s = self.rssm.obs_step(h, s, obs_seq[t], a_prev)\n",
                "\n",
                "            recons.append(self.decoder(h, s))       # (B,64,64,3)\n",
                "            rewards.append(self.reward_head(h, s))  # (B,1)\n",
                "            p_means.append(p_m); p_stds.append(p_s)\n",
                "            q_means.append(q_m); q_stds.append(q_s)\n",
                "            h_all.append(h);     s_all.append(s)\n",
                "\n",
                "        # Latent overshooting KL — computed across all (t, depth d) pairs\n",
                "        os_kl = []\n",
                "        for t in range(T - 1):\n",
                "            hi = h_all[t].detach()  # detach: overshooting does not backprop\n",
                "            si = s_all[t].detach()  # through the main sequence\n",
                "            D  = min(self.overshoot_d, T - 1 - t)\n",
                "            for d in range(1, D + 1):\n",
                "                im_m, im_s, hi, si = self.rssm.imagine_step(hi, si, action_seq[t+d])\n",
                "                tgt_m = q_means[t+d].detach()\n",
                "                tgt_s = q_stds[t+d].detach()\n",
                "                # KL[ imagined || posterior_target ]  (closed form for Gaussians)\n",
                "                kl = (torch.log(im_s / tgt_s)\n",
                "                      + (tgt_s**2 + (tgt_m - im_m)**2) / (2 * im_s**2) - 0.5)\n",
                "                os_kl.append(kl.sum(-1).mean())\n",
                "\n",
                "        overshoot_kl = (torch.stack(os_kl).mean()\n",
                "                        if os_kl else torch.tensor(0., device=device))\n",
                "\n",
                "        return (torch.stack(recons), torch.stack(rewards),\n",
                "                torch.stack(p_means), torch.stack(p_stds),\n",
                "                torch.stack(q_means), torch.stack(q_stds),\n",
                "                overshoot_kl)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 8: Loss + Train Step ─────────────────────────────────────────────────\n",
                "#\n",
                "# Three loss terms (Eq. 1 of the paper):\n",
                "#\n",
                "#  1. Reconstruction loss  — MSE between decoded and real pixels.\n",
                "#     Summed over pixels, averaged over batch+time.\n",
                "#     This is the *primary* signal that teaches the encoder/decoder to\n",
                "#     compress and reconstruct the visual world.\n",
                "#\n",
                "#  2. Reward loss  — MSE between predicted and real reward scalars.\n",
                "#     Keeps the latent state task-relevant (not just visually faithful).\n",
                "#\n",
                "#  3. KL divergence  — Encourages the prior p(s|h) to track the posterior\n",
                "#     q(s|h,e).  This is what makes the prior usable at planning time.\n",
                "#     beta scales the KL to prevent it from dominating early training.\n",
                "#     beta_os applies the same weighting to the overshooting KL.\n",
                "\n",
                "def planet_loss(recon, obs, pred_r, true_r, p_m, p_s, q_m, q_s, os_kl,\n",
                "                beta=0.1, beta_os=0.1):\n",
                "    # Recon: sum over spatial dims → per-frame scalar, then average\n",
                "    recon_loss  = F.mse_loss(obs, recon, reduction='none').sum(dim=[-1,-2,-3]).mean()\n",
                "    reward_loss = F.mse_loss(true_r.unsqueeze(-1), pred_r).mean()\n",
                "    # Closed-form KL between two diagonal Gaussians: q || p\n",
                "    kl = (torch.log(p_s / q_s)\n",
                "          + (q_s**2 + (q_m - p_m)**2) / (2 * p_s**2) - 0.5).sum(-1).mean()\n",
                "    return recon_loss + reward_loss + beta * kl + beta_os * os_kl\n",
                "\n",
                "\n",
                "def train_step(model, optimizer, obs, action, reward, device):\n",
                "    \"\"\"One gradient update.  Clips grad norm to prevent exploding gradients.\"\"\"\n",
                "    recon, pred_r, p_m, p_s, q_m, q_s, os_kl = model(obs, action)\n",
                "    loss = planet_loss(recon, obs, pred_r, reward, p_m, p_s, q_m, q_s, os_kl)\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    clip_grad_norm_(model.parameters(), max_norm=100.0)\n",
                "    optimizer.step()\n",
                "    return loss.item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 9: CEM Planner ───────────────────────────────────────────────────────\n",
                "#\n",
                "# Cross-Entropy Method (CEM) planning — no policy gradient, no value function.\n",
                "#\n",
                "# Algorithm (per action selection):\n",
                "#   1. Sample num_candidates action sequences from a Gaussian N(mu, std)\n",
                "#   2. Roll each sequence forward in LATENT SPACE (imagine_step × n_steps)\n",
                "#   3. Score each sequence by summing predicted rewards\n",
                "#   4. Refit mu and std to the top_k scoring sequences\n",
                "#   5. Repeat n_iter times (iterative refinement)\n",
                "#   6. Execute only the FIRST action from the best sequence (receding horizon)\n",
                "#\n",
                "# CEM operates entirely in latent space, so each evaluation costs only a\n",
                "# forward pass through the GRU + Prior + Reward — no pixel rendering needed.\n",
                "\n",
                "class CEMPlanner:\n",
                "    def __init__(self, model, num_candidates=1000, top_k=100,\n",
                "                 n_steps=12, n_iter=10, action_dim=1):\n",
                "        self.model          = model\n",
                "        self.num_candidates = num_candidates  # random action sequences per step\n",
                "        self.top_k          = top_k           # elite fraction for refitting\n",
                "        self.n_steps        = n_steps         # planning horizon\n",
                "        self.n_iter         = n_iter          # CEM refinement iterations\n",
                "        self.action_dim     = action_dim\n",
                "\n",
                "    @torch.no_grad()\n",
                "    def plan(self, h, s, device):\n",
                "        # Initialise Gaussian belief over action sequences\n",
                "        mu  = torch.zeros(self.n_steps, self.action_dim, device=device)\n",
                "        std = torch.ones_like(mu)\n",
                "\n",
                "        for _ in range(self.n_iter):\n",
                "            # Sample: (num_candidates, n_steps, action_dim)\n",
                "            acts = (mu + std * torch.randn(self.num_candidates, self.n_steps,\n",
                "                                           self.action_dim, device=device)).clamp(-1, 1)\n",
                "\n",
                "            # Broadcast h, s to batch size = num_candidates\n",
                "            H = h.expand(self.num_candidates, -1)  # (K, 200)\n",
                "            S = s.expand(self.num_candidates, -1)  # (K, 30)\n",
                "            G = torch.zeros(self.num_candidates, device=device)  # cumulative reward\n",
                "\n",
                "            for t in range(self.n_steps):\n",
                "                # Imagine one step forward using the prior (no obs)\n",
                "                _, _, H, S = self.model.rssm.imagine_step(H, S, acts[:, t])\n",
                "                G         += self.model.reward_head(H, S).squeeze(-1)\n",
                "\n",
                "            # Select top_k elite sequences and refit Gaussian\n",
                "            top = G.topk(self.top_k).indices\n",
                "            mu  = acts[top].mean(0)\n",
                "            std = acts[top].std(0).clamp(min=1e-4)\n",
                "\n",
                "        return mu[0]  # first action of the refined best sequence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 10: Episode Dataset ──────────────────────────────────────────────────\n",
                "#\n",
                "# Why cache episodes in RAM instead of reading from Drive each step?\n",
                "# ──────────────────────────────────────────────────────────────────\n",
                "# With virtual_len = batch_size × train_steps (e.g. 800 samples), the DataLoader\n",
                "# would make 800 Drive file-reads per iteration at ~10 ms/read = 8 seconds of\n",
                "# pure I/O before one gradient step.  Loading all episodes once at iteration\n",
                "# start costs the same total I/O but amortises it over the full iteration.\n",
                "#\n",
                "# virtual_len trick\n",
                "# ──────────────────\n",
                "# The DataLoader needs __len__ to know how many batches to produce.  If we\n",
                "# returned len(episodes) the loader would produce at most 1 batch when only 5\n",
                "# episodes exist (because 5 < batch_size=16).  By reporting a virtual length\n",
                "# equal to batch_size × train_steps we always get exactly train_steps batches.\n",
                "# __getitem__ ignores the DataLoader index and samples randomly instead.\n",
                "\n",
                "class EpisodeDataset(Dataset):\n",
                "    def __init__(self, storage, seq_len, virtual_len):\n",
                "        self.seq_len = seq_len\n",
                "        self.virtual_len = virtual_len\n",
                "        self.eps = []\n",
                "        for path in storage.episode_paths:\n",
                "            with np.load(path) as ep:\n",
                "                if len(ep['reward']) >= seq_len:\n",
                "                    # np.array() ensures a fresh, writeable, contiguous buffer\n",
                "                    # (NpzFile arrays are read-only; torch.from_numpy needs write access)\n",
                "                    self.eps.append({k: np.array(ep[k]) for k in ep})\n",
                "        if not self.eps:\n",
                "            raise RuntimeError('No episodes >= seq_len. Collect more data first.')\n",
                "        print(f'Dataset: {len(self.eps)} episodes cached in RAM')\n",
                "\n",
                "    def __len__(self):  return self.virtual_len\n",
                "\n",
                "    def __getitem__(self, _):\n",
                "        # Two levels of randomness: which episode, and which start frame\n",
                "        ep = self.eps[np.random.randint(len(self.eps))]\n",
                "        T  = len(ep['reward'])\n",
                "        i  = np.random.randint(0, T - self.seq_len + 1)\n",
                "        # Normalise obs from uint8 [0,255] → float32 [0,1]\n",
                "        obs = ep['obs'][i:i+self.seq_len].astype(np.float32) / 255.0\n",
                "        return (torch.from_numpy(obs),\n",
                "                torch.from_numpy(ep['action'][i:i+self.seq_len]),\n",
                "                torch.from_numpy(ep['reward'][i:i+self.seq_len]),\n",
                "                torch.from_numpy(ep['terminal'][i:i+self.seq_len].astype(np.float32)))\n",
                "\n",
                "\n",
                "def collate_fn(batch):\n",
                "    \"\"\"Stack items along batch dim and permute to (T, B, …).\n",
                "    WorldModel.forward loops over the time axis (dim 0), so sequences\n",
                "    must be (T, B, …) not the default PyTorch (B, T, …).\n",
                "    \"\"\"\n",
                "    obs, act, rew, term = zip(*batch)\n",
                "    return (torch.stack(obs).permute(1,0,2,3,4),   # (T,B,64,64,3)\n",
                "            torch.stack(act).permute(1,0,2),         # (T,B,action_dim)\n",
                "            torch.stack(rew).permute(1,0),            # (T,B)\n",
                "            torch.stack(term).permute(1,0))           # (T,B)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 11: Instantiate Everything ──────────────────────────────────────────\n",
                "base_env = gym.make('Pendulum-v1', render_mode='rgb_array')\n",
                "env      = PixelWrapper(base_env, render_size=64)\n",
                "\n",
                "model     = WorldModel(overshoot_d=5).to(config.device)\n",
                "optimizer = optim.AdamW(model.parameters(), lr=config.lr)\n",
                "planner   = CEMPlanner(model, action_dim=config.action_dim)\n",
                "storage   = EpisodeStorage(config.episode_dir)\n",
                "\n",
                "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 12: Training Loop ────────────────────────────────────────────────────\n",
                "#\n",
                "# Each iteration follows the PlaNet cycle:\n",
                "#   a. Load all episodes from Drive into RAM\n",
                "#   b. Run train_steps gradient updates on random subsequences\n",
                "#   c. Collect one new episode with the improved CEM planner\n",
                "#   d. Optionally checkpoint to Drive\n",
                "#\n",
                "# load_checkpoint resumes automatically if a .pt file exists on Drive;\n",
                "# returns 0 (start from scratch) otherwise — no explicit if-guard needed.\n",
                "\n",
                "start_iter = load_checkpoint(model, optimizer, config)\n",
                "device     = config.device\n",
                "\n",
                "# Seed with random episodes only on the very first run\n",
                "if len(storage) == 0:\n",
                "    seed_storage(env, storage, config.seed_episodes)\n",
                "\n",
                "for iteration in range(start_iter, config.total_iterations):\n",
                "    print(f'\\nIteration {iteration+1}/{config.total_iterations}  | Episodes: {len(storage)}')\n",
                "\n",
                "    # ── a. Build DataLoader from current Drive episodes ───────────────────\n",
                "    virtual_len = config.batch_size * config.train_steps_per_iter\n",
                "    dataset = EpisodeDataset(storage, config.seq_len, virtual_len)\n",
                "    loader  = DataLoader(\n",
                "        dataset, batch_size=config.batch_size,\n",
                "        shuffle=False,    # randomness is in __getitem__\n",
                "        num_workers=0,    # 0 avoids fork-deadlock in Colab\n",
                "        collate_fn=collate_fn, pin_memory=False, drop_last=True,\n",
                "    )\n",
                "\n",
                "    # ── b. Gradient updates ───────────────────────────────────────────────\n",
                "    model.train()\n",
                "    total_loss, steps = 0.0, 0\n",
                "    for obs_b, act_b, rew_b, _ in loader:\n",
                "        if steps >= config.train_steps_per_iter:\n",
                "            break\n",
                "        total_loss += train_step(model, optimizer,\n",
                "                                 obs_b.to(device), act_b.to(device), rew_b.to(device), device)\n",
                "        steps += 1\n",
                "    print(f'  Avg Loss: {total_loss / max(steps,1):.4f}  ({steps} steps)')\n",
                "\n",
                "    # ── c. Collect one new CEM episode ────────────────────────────────────\n",
                "    collect_episode(env, model, planner, storage, config)\n",
                "\n",
                "    # ── d. Checkpoint ─────────────────────────────────────────────────────\n",
                "    if (iteration + 1) % config.checkpoint_every == 0:\n",
                "        save_checkpoint(model, optimizer, iteration, config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 13: Evaluate ─────────────────────────────────────────────────────────\n",
                "# run_eval runs 5 episodes with the CEM planner and prints per-episode rewards.\n",
                "# Reward range for Pendulum-v1:  ~-1600 (random)  →  ~-150 (well-trained)\n",
                "avg_reward = run_eval(env, model, planner, n_episodes=5, config=config)\n",
                "save_checkpoint(model, optimizer, config.total_iterations - 1, config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Cell 14: Dream Visualizer ─────────────────────────────────────────────────\n",
                "# visualize_dream shows how well the model can imagine the future:\n",
                "#   • Green (CONTEXT): real frames fed via encoder — warms up latent state\n",
                "#   • Red   (DREAM):   prior rolls out with NO real pixels — pure imagination\n",
                "#\n",
                "# Good model: dream frames match real frames for many steps\n",
                "# Weak model: dream frames blur or drift quickly after context ends\n",
                "print('Generating dream visualisations...')\n",
                "for ep_idx in range(min(3, len(storage))):\n",
                "    visualize_dream(model, storage, config,\n",
                "                    episode_idx=ep_idx, context_frames=5, dream_steps=50)\n",
                "print(f'Saved to: {config.viz_dir}')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}